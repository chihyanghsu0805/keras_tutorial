#   Linear Models:

-   `Linear Regression` (Exponential family) follows the form h<sub>θ</sub>(x) = θ<sup>T</sup>x with x<sub>0</sub> being 1 and θ<sub>0</sub> being the bias term. It commonly uses `Minimum Squared Error` (MSE) as the Loss term, (y -h<sub>θ</sub>(x))<sup>2</sup> which can be derived from `Maximum Likelihood Estimation (MLE)` with `Gaussian` distribution. Besides iterative methods, Linear Regression can also be solved using analytical methods such as  Normal equation to find θ* in one step with θ* = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y, given that (X<sup>T</sup>X) is invertible. Note that matrix inverses are expensive operation and prone to numerical errors.

-   `Locally Weighted Linear Regression` (Non-Parametric) weighs the individual loss by non-negative function w(i) based on their distance to the point of prediction.

-   `Logistic Regression` (Exponential family) can be used to predict probabilities and is often used for `binary classification` with probability threshold. It follows the form h<sub>θ</sub>(x) = σ(z) with σ as the `sigmoid/logistic` function, σ (z) = 1/(1+e<sup>-z</sup>) and z = θ<sup>T</sup>x. The probability resembles the `Bernoulli` distribution, P(y = x; θ) = h<sub>θ</sub>(x) <sup>y</sup> (1 - h<sub>θ</sub>(x)) <sup> (1-y) </sup>. `Regularization` is `extremely important` for Logistic Regression as the sigmoid function reaches asymptotic when z approaches +/- infinity. Without regularization, the parameters may explode and/or vanish. The loss function is the product of all examples Π P(y = x; θ) and is usually applied by a logarithmic to become `log likelihood` which becomes sum of all examples, Σ P(y = x; θ) = Σ y log(h<sub>θ</sub>(x)) + (1-y) log((1 - h<sub>θ</sub>(x))). This loss is also know as `cross entropy loss (XEnt)`. `Maximum Likelihood Estimation` maximizes the log likelihood with gradient ascent, but is usually transformed to `minimizing negative log likelihood` with `gradient descent`.

-   `Softmax Regression` is logistic regression with `multi-class` classification, or `multinoulli distribution`. The logits (z, θ<sup>T</sup>x) are exponentialized and normalized so the probabilities sums to 1, (e<sup>θ<sub>i</sub><sup>T</sup>x</sup> / 	Σ<sub>j</sub> e<sup>θ<sub>j</sub><sup>T</sup>x</sup>). The loss function become `categorical cross entropy`. For problems with `multi-class single instance`, softmax is used and `multi-class multi-instance`, logistic regression is used for each instance. Additionally, a `temperature, t` term can be added to softmax so the outputput is less hard, e<sup>θ<sub>i</sub><sup>T</sup>x / t</sup> / 	Σ<sub>j</sub> e<sup>θ<sub>j</sub><sup>T</sup>x / t</sup>.

-   `Exponential Family` is a family of models that can be factored into P(y; η) = b(y) exp(η<sup>T</sup>T(y)-α(η)), where α is log partition. Examples are `Gaussian`, `Bernoulli`, Poisson, Gamma, Exponential, Beta, Dirichlet. MLE of Exponential Family is always concave, so covergence is guaranteed with `random initialization`. `Generalized Linear Models (GLMs)` are a set of models that follows the properties:
    -   y | x; θ ~ Exponential Family
    -   η = θ<sup>T</sup>x  (linear)
    -   At train time, maximizes log P(y; η). At test time, outputs E[y; η] = h<sub>θ</sub>(x).
    -   Learning update rule: θ<sub>j</sub> = θ<sub>j</sub> + α (y<sup>i</sup> - h<sub>θ</sub>(x<sup>i</sup>)) x<sup>i</sup><sub>j</sub>, where α is learning rate.
