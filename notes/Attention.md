# Attention

https://lilianweng.github.io/posts/2018-06-24-attention/

Attention mechanisms are used to help with long range dependency problems in Neural Networks.

It computes the correlation between two sequences.

There are different attention mechanisms,

- Additive (Concat)
- Dot Product
- Scaled Dot Product

It can also be categorized into,

- Self-Attention: where the sequences are identical
- Global Attention: attention to the entire sequence
- Local Attention: attention to a subset of the sequence
- Soft Attention
- Hard Attention

## Transformers
