# This folder contains Language Model papers.

- RoBERTa: ARobustly Optimized BERT
  - https://arxiv.org/abs/1907.11692

- DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
  - https://arxiv.org/abs/1910.01108

- Large Batch Optimization for Deep Learning: Training BERT in 76 minutes
  - https://arxiv.org/abs/1904.00962

- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
  - https://arxiv.org/abs/1810.04805

- Neural Machine Translation of Rare Words with Subword Units
  - https://arxiv.org/abs/1508.07909
