This folder contains papers in deep learning.
Related works are grouped in individual folders.

TMLR:
-   Emergent Abilities of Large Language Models

JMLR:
-   Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
    -   https://arxiv.org/abs/1910.10683
    -   https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html

ECCV:
-   2020:
    -   Big Transfer (BiT): General Visual Representation Learning
        -   https://arxiv.org/abs/1912.11370
-   2016:
    -   Deep Networks with Stochastic Depth
        -   https://arxiv.org/abs/1603.09382

EMNLP:
-   2022:
    -   "Will You Find These Shortcuts?" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification
        -   https://arxiv.org/abs/2111.07367
        -   https://ai.googleblog.com/2022/12/will-you-find-these-shortcuts.html

ICLR:
-   2022:
    -   ViDT: An Efficient and Effective Fully Transformer-based Object Detector
        -   https://arxiv.org/abs/2110.03921
    
    -   Learning Strides in Convolutional Neural Networks
        -   https://arxiv.org/abs/2202.01653
        -   https://github.com/google-research/diffstride
    
    - ViTGAN: Training GANs with Vision Transformers
        -   https://arxiv.org/abs/2107.04589        -   
    
    - PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions
        -   https://arxiv.org/abs/2204.12511
        
    - Natural Language Descriptions of Deep Visual Features
        - https://arxiv.org/abs/2201.11114
        
    -   Pix2Seq: A Language Modeling Framework for Object Detection
        -   https://arxiv.org/pdf/2109.10852.pdf

-   2021:
    -   Deformable DETR: Deformable Transformers for End-to-End Object Detection
        -   https://arxiv.org/pdf/2010.04159.pdf

- 2018:
    -   Spectral Normalization for Generative Adversarial Networks
        -   https://arxiv.org/abs/1802.05957
    - Graph Attention Networks
        - https://arxiv.org/abs/1710.10903
        - https://github.com/PetarV-/GAT

CogSci:
- 2021:
    - Are Convolutional Neural Networks or Transformers more like human vision?
        - https://arxiv.org/abs/2105.07197

ICCV:
- 2021:
    - Explaining in Style: Training a GAN to explain a classifier in StyleSpace
        - https://arxiv.org/pdf/2104.13369.pdf
        - https://explaining-in-style.github.io/

    -   Deformable Convolutional Networks
        -   https://github.com/msracver/Deformable-ConvNets
        -   https://arxiv.org/abs/1703.06211
        -   
NIPS:
-   2022:
    -   Confident Adaptive Language Modeling
        -   https://arxiv.org/abs/2207.07061
        -   https://github.com/google-research/t5x/tree/main/t5x/contrib/calm
        -   https://ai.googleblog.com/2022/12/accelerating-text-generation-with.html

    -   Chain of Thought Prompting Elicits Reasoning in Large Language Models
        -   https://arxiv.org/abs/2201.11903
        -   https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html

    -   Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding
        -   https://arxiv.org/abs/2205.11487

- 2021:
    - Do Vision Transformers See Like Convolutional Neural Networks?
        - https://arxiv.org/abs/2108.08810

    - MLP-Mixer: An all-MLP Architecture for Vision
        - https://arxiv.org/pdf/2105.01601.pdf
        - https://github.com/google-research/vision_transformer

-   2020:
    -   Debugging Tests for Model Explanations
        -   https://arxiv.org/abs/2011.05429

    -   Denoising Diffusion Probabilistic Models
        -   https://arxiv.org/abs/2006.11239
        -   https://github.com/hojonathanho/diffusion

CVPR:
-   2022:
    -   LiT: Zero-Shot Transfer with Locked-image text Tuning
        -   https://arxiv.org/abs/2111.07991

- 2020:
    - Self-training with Noisy Student improves ImageNet classification
        - https://arxiv.org/abs/1911.04252
        - https://github.com/google-research/noisystudent

    - Learning Texture Transformer Network for Image Super-Resolution
        - https://arxiv.org/abs/2006.04139
        - https://github.com/researchmm/TTSR

    - Bringing Old Photos Back to Life
        - https://arxiv.org/abs/2004.09484
        - https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life

    -   RandAugment: Practical automated data augmentation with a reduced search space
        -   https://arxiv.org/abs/1909.13719
        -   https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet

- 2016:
    - Learning Deep Features for Discriminative Localization
        - https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf
        - https://github.com/zhoubolei/CAM

- Scaling SGD Batch Size to 32K for ImageNet Training
    - https://arxiv.org/abs/1708.03888v1?2

- Scaling SGD Batch Size to 32K for ImageNet Training
    - https://arxiv.org/abs/1708.03888v1?2



Google AI Blog:
-   Accurate Alpha Matting for Portrait Mode Selfies on Pixel 6
    -   https://ai.googleblog.com/2022/01/accurate-alpha-matting-for-portrait.html

-   Transcending Scaling Laws with 0.1% Extra Compute
    -   https://ai.googleblog.com/2022/11/better-language-models-without-massive.html
    -   https://arxiv.org/abs/2210.11399

-   Scaling Instruction-Finetuned Language Models
    -   https://ai.googleblog.com/2022/11/better-language-models-without-massive.html
    -   https://arxiv.org/abs/2210.11416

-   Transforming Sequence Tagging Into A Seq2Seq Task
    -   https://arxiv.org/abs/2203.08378

-   CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning
    -   https://arxiv.org/abs/2112.08558

-   PaLM: Scaling Language Modeling with Pathways
    -   https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html
    -   https://arxiv.org/abs/2204.02311

-   LaMDA:LanguageModelsforDialogApplications
    -   https://arxiv.org/abs/2201.08239

Amazon AI BLog:
-   Dialogue Meaning Representation for Task-Oriented Dialogue Systems
    -   https://arxiv.org/abs/2204.10989
    -   https://github.com/amazon-science/dialogue-meaning-representation
