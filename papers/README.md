# Papers

[Active Learning](./README.md#active-learning) </br>
[Contrastive Learning](./contrastive_learning/)
[Computer Vision](./README.md#computer-vision) </br>
[DCGAN](./README.md#dcgan) </br>
[Deeplab](./README.md#deeplab) </br>
[DETR](./README.md#detr) </br>
[Dialogue](./README.md#dialogue) </br>
[Few Shot Learning](./README.md#few-shot-learning) </br>
[GPT](./README.md#gpt) </br>
[GAN](./README.md#gan) </br>
[Image-to-Image Translation](./README.md#image-to-image-translation) </br>
[Inception](./README.md#inception) </br>
[Knowledge Distillation](./README.md#knowledge-distillation) </br>
[Language Models](./README.md#language-models) </br>
[Model Interpretation](./README.md#model-interpretation) </br>
[Natural Language Processing](./README.md#natural-language-processing) </br>
[Normalization](./README.md#normalization) </br>
[RCNN](./README.md#rcnn) </br>
[Resnet](./README.md#resnet) </br>
[SAGAN](./README.md#sagan) </br>
[SimCLR](./README.md#simclr) </br>
[Stable Diffusion](./README.md#stable-diffusion) </br>
[StyleGAN](./README.md#stylegan) </br>
[Super Resolution](./README.md#super-resolution) </br>
[SWIN](./README.md#swin) </br>
[Transformers](./README.md#transformers) </br>
[UNETR](./README.md#unetr) </br>
[UNet](./README.md#unet) </br>
[ViT](./README.md#vit) </br>
[WGAN](./README.md#wgan) </br>
[YOLO](./README.md#yolo) </br>

---

## Active Learning

-   Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks
    -   https://arxiv.org/abs/1807.07356
    -   NeuroComputing

-   Exploring uncertainty measures in deep networks for multiple sclerosis lesion detection and segmentation
    -   https://arxiv.org/abs/1808.01200
    -   MICCAI 2018

-   MONAI Label: A framework for AI-assisted Interactive Labeling of 3D Medical Images
    -   https://arxiv.org/abs/2203.12362

-   https://internlp.github.io/accepted_papers.html

## Computer Vision

-   Emerging Properties in Self-Supervised Vision Transformers
    -   https://arxiv.org/abs/2104.14294
    -   ICCV 2021
    -   https://github.com/facebookresearch/dino
    -   https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/

-   Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples
    -   https://arxiv.org/abs/2104.13963
    -   ICCV 2021
    -   https://github.com/facebookresearch/suncet
    -   https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/


# Dialogue

-   Dialogue Meaning Representation for Task-Oriented Dialogue Systems
    -   https://arxiv.org/abs/2204.10989
    -   EMNLP 2022
    -   https://github.com/amazon-science/dialogue-meaning-representation
    -   [Notes](./2204.10989.md)

-   Semantic Parsing for Task Oriented Dialog using Hierarchical Representations
    -   https://arxiv.org/abs/1810.07942
    -   EMNLP 2018
    -   http://fb.me/semanticparsingdialog

-   Conversational Semantic Parsing for Dialog State Tracking
    -   https://github.com/apple/ml-tree-dst
    -   EMNLP 2020
    -   https://arxiv.org/abs/2010.12770

-   Abstract Meaning Representation for Sembanking
    -   https://aclanthology.org/W13-2322.pdf

-   Semantic Representation for Dialogue Modeling
    -   https://arxiv.org/abs/2105.10188
    -   ACL 2021

-   Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence
    -   https://arxiv.org/abs/2210.07109
    -   EMNLP 2022

# Natural Language Processing

-   Emergent Abilities of Large Language Models
    -   https://arxiv.org/abs/2206.07682
    -   TMLR

-   LaMDA: Language Models for Dialog Applications
    -   https://arxiv.org/abs/2201.08239
    -   https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html

# Few Shot Learning

-   Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
    -   https://arxiv.org/abs/1703.03400
    -   ICML 2017

-   On First-Order Meta-Learning Algorithms
    -   https://arxiv.org/abs/1803.02999
    -   https://github.com/openai/supervised-reptile

# GAN
-   Generative Adversarial Networks
    -   https://arxiv.org/abs/1406.2661
    -   NIPS 2014

-   NIPS 2016 Tutorial: Generative Adversarial Networks
    -   https://arxiv.org/abs/1701.00160
    -   NIPS 2016

-   Improved Techniques for Training GANs
    -   https://arxiv.org/abs/1606.03498
    -   NIPS 2016

-   Least Squares Generative Adversarial Networks
    -   https://arxiv.org/abs/1611.04076
    -   ICCV 2017

# WGAN
-   Wasserstein GAN
    -   https://arxiv.org/abs/1701.07875

-   Improved Training of Wasserstein GANs
    -   https://arxiv.org/abs/1704.00028
    -   NIPS 2017

# DCGAN
-   Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks
    -   https://arxiv.org/abs/1511.06434
    -   ICLR 2016

# SAGAN
-   Self-Attention Generative Adversarial Networks
    -   https://arxiv.org/abs/1805.08318
    -   ICML 2019
    -   https://github.com/brain-research/self-attention-gan

# GPT
-   Improving Language Understanding by Generative Pre-Training
    -   https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
    -   https://openai.com/blog/language-unsupervised/

-   Language Models are Unsupervised Multitask Learners
    -   https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
    -   https://github.com/openai/gpt-2
    -   https://openai.com/blog/better-language-models/

-   Language Models are Few-Shot Learners
    -   https://arxiv.org/abs/2005.14165
    -   NIPS 2020

-   Generative Pretraining from Pixels
    -   http://proceedings.mlr.press/v119/chen20s/chen20s.pdf
    -   ICML 2020
    -   https://github.com/openai/image-gpt
    -   https://openai.com/blog/image-gpt/

-   The Pile: An 800GB Dataset of Diverse Text for Language Modeling
    -   https://arxiv.org/abs/2101.00027
    -   https://github.com/EleutherAI/gpt-neo

-   Mesh Transformer JAX
    -   https://www.eleuther.ai/projects/mesh-transformer-jax/

-   GPT-NeoX-20B: An Open-Source Autoregressive Language Model
    -   https://arxiv.org/abs/2204.06745
    -   ACL 2022
    -   https://github.com/EleutherAI/gpt-neox

# Image-to-Image Translation

-   Image-to-Image Translation with Conditional Adversarial Networks
    -   https://arxiv.org/abs/1611.07004
    -   2017 CVPR

-   Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks
    -   https://arxiv.org/abs/1703.10593
    -   2017 CVPR
    -   https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix

-   High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs
    -   https://arxiv.org/abs/1711.11585
    -   2018 CVPR
    -   https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix

-   Contrastive Learning for Unpaired Image-to-Image Translation
    -   https://arxiv.org/pdf/2007.15651
    -   2020 ECCV
    -   https://github.com/taesungp/contrastive-unpaired-translation

# Inception

-   Rethinking the Inception Architecture for Computer Vision
    -   https://arxiv.org/abs/1512.00567
    -   2016 CVPR

-   Going Deeper with Convolutions
    -   https://arxiv.org/abs/1409.4842
    -   2015 CVPR

# Knowledge Distillation

- Distilling the Knowledge in a Neural Network
  - https://arxiv.org/abs/1503.02531

# Language Models

-   BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
    -   https://arxiv.org/abs/1910.13461
    -   https://github.com/facebookresearch/fairseq

-   RoBERTa: A Robustly Optimized BERT
    -   https://arxiv.org/abs/1907.11692
    -   https://github.com/facebookresearch/fairseq

-   DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
    -   https://arxiv.org/abs/1910.01108
    -   NIPS 2019

-   Large Batch Optimization for Deep Learning: Training BERT in 76 minutes
    -   https://arxiv.org/abs/1904.00962
    -   ICLR 2020

-   BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
    -   https://arxiv.org/abs/1810.04805

#

-   Neural Machine Translation of Rare Words with Subword Units
    -   https://arxiv.org/abs/1508.07909
    -   ACL 2016

# Model Interpretation

-   Learning Deep Features for Discriminative Localization
    -   https://arxiv.org/pdf/1512.04150.pdf
    -   CVPR 2016

-   Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps
    -   https://arxiv.org/abs/1312.6034
    -   ICLR 2014

-   Visualizing and Understanding Convolutional Networks
    -   https://arxiv.org/abs/1311.2901
    -   ECCV 2014

-   Understanding Neural Networks Through Deep Visualization
    -   https://arxiv.org/abs/1506.06579
    -   ICML 2015

-   Axiomatic Attribution for Deep Networks
    -   https://arxiv.org/abs/1703.01365
    -   ICML 2017

# Normalization

-   Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
    -   https://arxiv.org/abs/1502.03167
    -   2015 ICML

-   Layer Normalization
    -   https://arxiv.org/abs/1607.06450

-   Group Normalization
    -   https://arxiv.org/abs/1803.08494
    -   ECCV 2018

-   Instance Normalization: The Missing Ingredient for Fast Stylization
    -   https://arxiv.org/abs/1607.08022
    -   https://github.com/DmitryUlyanov/texture_nets

-   Micro-Batch Training with Batch-Channel Normalization and Weight Standardization
    -   https://arxiv.org/abs/1903.10520

-   Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks
    -   https://arxiv.org/pdf/1602.07868.pdf
    -   NIPS 2016

# RCNN

-   Rich feature hierarchies for accurate object detection and semantic segmentation
    -   https://arxiv.org/abs/1311.2524

- Fast R-CNN
    -   https://arxiv.org/abs/1504.08083
    -   ICCV 2015

-   Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks
    -   https://arxiv.org/abs/1506.01497
    -   NIPS 2015

-   Mask R-CNN
    -   https://arxiv.org/abs/1703.06870
    -   ICCV 2017

-   Focal Loss for Dense Object Detection
    -   https://arxiv.org/abs/1708.02002
    -   ICCV 2017

-   Feature Pyramid Networks for Object Detection
    -   https://arxiv.org/abs/1612.03144
    -   CVPR 2017

# Resnet

-   Aggregated Residual Transformations for Deep Neural Networks
    -   CVPR 2017
    -   https://arxiv.org/pdf/1611.05431

-   Deep Residual Learning for Image Recognition
    -   https://arxiv.org/abs/1512.03385
    -   CVPR 2016

-   Densely Connected Convolutional Networks
    -   https://arxiv.org/abs/1608.06993
    -   CVPR 2017

-   Highway Networks
    -   https://arxiv.org/abs/1505.00387
    -   ICML 2015

-   Identity Mappings in Deep Residual Networks
    -   https://arxiv.org/abs/1603.05027
    -   ECCV 2016

-   Residual Networks Behave Like Ensembles of Relatively Shallow Networks
    -   https://arxiv.org/abs/1605.06431
    -   NIPS 2016

# Deeplab

-   CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation
    -   https://arxiv.org/pdf/2206.08948.pdf
    -   CVPR 2022

-   Rethinking Atrous Convolution for Semantic Image Segmentation
    -   https://arxiv.org/abs/1706.05587

-   DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs
    -   https://ieeexplore.ieee.org/abstract/document/7913730
    -   IEEE transactions on pattern analysis and machine intelligence 2017
    -   http://liangchiehchen.com/projects/DeepLab.html#download

-   Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs
    -   https://arxiv.org/abs/1412.7062
    -   ICLR 2015

# UNet

-   Attention U-Net: Learning Where to Look for the Pancreas
    -   https://arxiv.org/abs/1804.03999
    -   MIDL 2018

-   Automated Design of Deep Learning Methods for Biomedical Image Segmentation
    -   https://arxiv.org/abs/1904.08128

-   Optimized U-Net for Brain Tumor Segmentation
    -   https://arxiv.org/abs/2110.03352

-   nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation
    -   https://arxiv.org/abs/1809.10486
    -   Nature Methods 2021

-   No New-Net
    -   https://arxiv.org/abs/1809.10483
    -   MICCAI 2018

-   U-Net: Convolutional Networks for Biomedical Image Segmentation
    -   https://arxiv.org/abs/1505.04597
    -   MICCAI 2015

# Stable Diffusion

-   High-Resolution Image Synthesis with Latent Diffusion Models
    -   https://arxiv.org/abs/2112.10752
    -   CVPR 2022
    -   https://github.com/CompVis/stable-diffusion
    -   https://ommer-lab.com/research/latent-diffusion-models/

-   Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding
    -   https://arxiv.org/abs/2205.11487
    -   https://imagen.research.google/

# StyleGAN

-   A Style-Based Generator Architecture for Generative Adversarial Networks
    -   https://arxiv.org/abs/1812.04948
    -   CVPR 2019
    -   https://github.com/NVlabs/stylegan

-   Analyzing and Improving the Image Quality of StyleGAN
    -   https://arxiv.org/abs/1912.04958
    -   CVPR 2020
    -   https://github.com/NVlabs/stylegan2

-   Alias-Free Generative Adversarial Networks
    -   https://arxiv.org/abs/2106.12423
    -   NIPS 2021
    -   https://github.com/NVlabs/stylegan3

# Super Resolution
-   ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks
    -   https://arxiv.org/abs/1809.00219
    -   ECCV 2018

-   Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network
    -   https://arxiv.org/abs/1609.04802
    -   CVPR 2017

# Transformers

-   Attention Is All You Need
    -   https://arxiv.org/abs/1706.03762
    -   NIPS 2017
    -   https://github.com/tensorflow/tensor2tensor

-   ViDT: An Efficient and Effective Fully Transformer-based Object Detector
    -   2022 ICLR
    -   https://arxiv.org/abs/2110.03921

# ViT

-   Training Data-Efficient Image Transformers & Distillation Through Attention
    -   ICML 2021
    -   https://arxiv.org/abs/2012.12877

-   How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers
    -   https://arxiv.org/abs/2106.10270

-   An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
    -   https://arxiv.org/pdf/2010.11929
    -   ICLR 2021
    -   https://github.com/google-research/vision_transformer

# DETR

-   Deformable DETR: Deformable Transformers for End-to-End Object Detection
  - ICLR 2021
  - https://arxiv.org/pdf/2010.04159.pdf

-   End-to-End Object Detection with Transformers
    -   https://arxiv.org/abs/2005.12872
    -   ECCV 2020
    -   https://github.com/facebookresearch/detr

# SWIN
-   Swin Transformer V2: Scaling Up Capacity and Resolution
    -   https://arxiv.org/abs/2111.09883
    -   CVPR 2022

-   Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    -   https://arxiv.org/abs/2103.14030
    -   ICCV 2021
    -   https://github.com/microsoft/Swin-Transformer

# UNETR
-   Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis
    -   https://arxiv.org/abs/2111.14791
    -   CVPR 2022
    -   https://github.com/Project-MONAI/research-contributions/tree/main/SwinUNETR/

-   Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images
    -   https://arxiv.org/abs/2201.01266
    -   https://github.com/Project-MONAI/research-contributions/tree/master/SwinUNETR

-   UNETR: Transformers for 3D Medical Image Segmentation
    -   https://arxiv.org/abs/2103.10504
    -   WACV 2022

# YOLO

-   You Only Look Once: Unified, Real-Time Object Detection
    -   https://arxiv.org/abs/1506.02640
    -   CVPR 2016

-   YOLO9000: Better, Faster, Stronger
    -   https://arxiv.org/abs/1612.08242
    -   CVPR 2017

-   YOLOv3: An Incremental Improvement
    -   https://arxiv.org/abs/1804.02767

-   YOLOv4: Optimal Speed and Accuracy of Object Detection
    -   https://arxiv.org/abs/2004.10934

# To be cleaned

-   Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
    -   https://arxiv.org/abs/1910.10683
    -   JMLR 2022?
    -   https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html

-   Big Transfer (BiT): General Visual Representation Learning
    -   https://arxiv.org/abs/1912.11370
    -   ECCV 2020

-   Deep Networks with Stochastic Depth
    -   https://arxiv.org/abs/1603.09382
        -   ECCV 2016

-   "Will You Find These Shortcuts?" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification
    -   https://arxiv.org/abs/2111.07367
    -   EMNLP 2022
    -   https://ai.googleblog.com/2022/12/will-you-find-these-shortcuts.html

-   Natural Language Descriptions of Deep Visual Features
    -   https://arxiv.org/abs/2201.11114
    -   ICLR 2022
-   Pix2Seq: A Language Modeling Framework for Object Detection
    -   https://arxiv.org/pdf/2109.10852.pdf
    -   ICLR 2022

-   Graph Attention Networks
    -   https://arxiv.org/abs/1710.10903
    -   ICLR 2018
    -   https://github.com/PetarV-/GAT

-   Are Convolutional Neural Networks or Transformers more like human vision?
    -   https://arxiv.org/abs/2105.07197
    -   CogSci 2021

-   Explaining in Style: Training a GAN to explain a classifier in StyleSpace
    -   https://arxiv.org/pdf/2104.13369.pdf
    -   ICCV 2021
    -   https://explaining-in-style.github.io/

-   Confident Adaptive Language Modeling
    -   https://arxiv.org/abs/2207.07061
    -   NIPS 2022
    -   https://github.com/google-research/t5x/tree/main/t5x/contrib/calm
    -   https://ai.googleblog.com/2022/12/accelerating-text-generation-with.html

-   Chain of Thought Prompting Elicits Reasoning in Large Language Models
    -   https://arxiv.org/abs/2201.11903
    -   NIPS 2022
    -   https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html

-   Do Vision Transformers See Like Convolutional Neural Networks?
    -   NIPS 2021
    -   https://arxiv.org/abs/2108.08810

-   MLP-Mixer: An all-MLP Architecture for Vision
    -   https://arxiv.org/pdf/2105.01601.pdf
    -   NIPS 2021
    -   https://github.com/google-research/vision_transformer

-   Debugging Tests for Model Explanations
    -   https://arxiv.org/abs/2011.05429
    -   NIPS 2020

-   Denoising Diffusion Probabilistic Models
    -   https://arxiv.org/abs/2006.11239
    -   NIPS 2020
    -   https://github.com/hojonathanho/diffusion

-   LiT: Zero-Shot Transfer with Locked-image text Tuning
    -   https://arxiv.org/abs/2111.07991
    -   CVPR 2022

-   Self-training with Noisy Student improves ImageNet classification
    -   https://arxiv.org/abs/1911.04252
    -   CVPR 2020
    -   https://github.com/google-research/noisystudent

-   Learning Texture Transformer Network for Image Super-Resolution
    -   https://arxiv.org/abs/2006.04139
    -   CVPR 2020
    -   https://github.com/researchmm/TTSR

-   Bringing Old Photos Back to Life
    -   https://arxiv.org/abs/2004.09484
    -   CVPR 2020
    -   https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life

-   RandAugment: Practical automated data augmentation with a reduced search space
    -   https://arxiv.org/abs/1909.13719
    -   CVPR 2020
    -   https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet

Google AI Blog:
-   Accurate Alpha Matting for Portrait Mode Selfies on Pixel 6
    -   https://ai.googleblog.com/2022/01/accurate-alpha-matting-for-portrait.html

-   Transcending Scaling Laws with 0.1% Extra Compute
    -   https://arxiv.org/abs/2210.11399
    -   https://ai.googleblog.com/2022/11/better-language-models-without-massive.html

-   Scaling Instruction-Finetuned Language Models
    -   https://arxiv.org/abs/2210.11416
    -   https://ai.googleblog.com/2022/11/better-language-models-without-massive.html

-   Transforming Sequence Tagging Into A Seq2Seq Task
    -   https://arxiv.org/abs/2203.08378

-   PaLM: Scaling Language Modeling with Pathways
    -   https://arxiv.org/abs/2204.02311
    -   https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html

-   Deep Communicating Agents for Abstractive Summarization
    -   https://arxiv.org/abs/1803.10357
    -   ACL 2018

-   A Deep Reinforced Model for Abstractive Summarization
    -   https://arxiv.org/abs/1705.04304

https://ai.googleblog.com/2022/12/who-said-what-recorders-on-device.html

##

https://larel-workshop.github.io/papers/

-   Knowledge-grounded Dialog State Tracking
    -   https://arxiv.org/abs/2210.06656
    -   AAAI 2022

-   QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation
    -   https://arxiv.org/abs/2210.15718
    -   EMNLP 2022

-   Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation
    -   https://arxiv.org/abs/2202.07654
    -   EMNLP 2022
    -   https://github.com/google-research-datasets/answer-equivalence-dataset

-   Exploring Dual Encoder Architectures for Question Answering
    -   https://arxiv.org/abs/2204.07120
    -   EMNLP 2022

-   CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning
    -   https://arxiv.org/abs/2112.08558
    -   EMNLP 2022

-   Learning Strides in Convolutional Neural Networks
    -   https://arxiv.org/abs/2202.01653
    -   ICLR 2022
    -   https://github.com/google-research/diffstride

Deformable Convolutional Networks
    -   https://github.com/msracver/Deformable-ConvNets
    -   ICCV 2021
    -   https://arxiv.org/abs/1703.06211


-   Deformable DETR: Deformable Transformers for End-to-End Object Detection
    -   ICLR 2021
    -   https://arxiv.org/pdf/2010.04159.pdf


-   Spectral Normalization for Generative Adversarial Networks
    -   ICLR 2018
    -   https://arxiv.org/abs/1802.05957

-   ViTGAN: Training GANs with Vision Transformers
    -   ICLR 2022
    -   https://arxiv.org/abs/2107.04589

-   PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions
    -   ICLR 2022
    -   https://arxiv.org/abs/2204.12511

-   Scaling SGD Batch Size to 32K for ImageNet Training
    -   CVPR 2016
    -   https://arxiv.org/abs/1708.03888v1?2
