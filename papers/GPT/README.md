# This folder contains papers for Generative Pre-Training (GPT)

- Improving Language Understanding by Generative Pre-Training
    - https://openai.com/blog/language-unsupervised/
    - https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf

- Better Language Models and Their Implications
    - https://openai.com/blog/better-language-models/
    - https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

- Language Models are Few-Shot Learners
    - https://arxiv.org/abs/2005.14165

- Image GPT
    - https://openai.com/blog/image-gpt/

- The Pile: An 800GB Dataset of Diverse Text for Language Modeling
    - https://arxiv.org/abs/2101.00027
    - https://github.com/EleutherAI/gpt-neo

- Mesh Transformer JAX
    - https://www.eleuther.ai/projects/mesh-transformer-jax/

- GPT-NeoX-20B: An Open-Source Autoregressive Language Model
    - https://arxiv.org/abs/2204.06745
