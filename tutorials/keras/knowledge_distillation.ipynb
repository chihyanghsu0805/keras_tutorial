{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chihyanghsu0805/machine_learning/blob/tutorials/tutorials/keras/knowledge_distillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook re-implements the tutorial on https://keras.io/examples/vision/knowledge_distillation/.\n",
        "\n",
        "The original paper is https://arxiv.org/abs/1503.02531.\n",
        "\n",
        "First, a `teacher` model is trained.\n",
        "\n",
        "Then, knowledge is distilled to the `student` model by training with a combined weighted loss function, \n",
        "\n",
        "1.   The first objective function is the cross entropy with the soft targets and this cross entropy is computed using the same high temperature in the softmax of the distilled model as was used for generating the soft targets from the cumbersome model. \n",
        "\n",
        "2.   The second objective function is the cross entropy with the correct labels. This is computed using exactly the same logits in softmax of the distilled model but at a temperature of 1. We found that the best results were generally obtained by using a condiderably lower weight on the second objective function. \n",
        "\n"
      ],
      "metadata": {
        "id": "-Y3IEtgLKdsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation with Teacher Student network"
      ],
      "metadata": {
        "id": "YtHLPL4fKsvA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzHT8FxiKRI7"
      },
      "source": [
        "## Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E7oqLbzSKRI7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYqV6O2IKRI_"
      },
      "source": [
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F1GzjY_GKRI_"
      },
      "outputs": [],
      "source": [
        "# Prepare the train and test dataset.\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
        "\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.reshape(x_test, (-1, 28, 28, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmFdumZBKRI8"
      },
      "source": [
        "## Distiller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u7hvgQLLKRI9"
      },
      "outputs": [],
      "source": [
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=3,\n",
        "    ):\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        #   One way to do this is to use the correct labels to modify the soft\n",
        "        #   targets, but we found that a better way is to simply use a weighted\n",
        "        #   average of two different objective functions. The first objective\n",
        "        #   function is the cross entropy with the soft targets and this\n",
        "        #   cross entropy is computed using the same high temperature in the\n",
        "        #   softmax of the distilled model as was used for generating the soft\n",
        "        #   targets from the cumbersome model. The second objective function is\n",
        "        #   the cross entropy with the correct labels. This is computed using\n",
        "        #   exactly the same logits in softmax of the distilled model but at a\n",
        "        #   temperature of 1. We found that the best results were generally\n",
        "        #   obtained by using a considerably lower weight on the second\n",
        "        #   objective function. \n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        #results.update(\n",
        "        #    {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        #)\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        y_prediction = self.student(x, training=False)\n",
        "\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        #results.update({\"student_loss\": student_loss})\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH43_V6JKRI-"
      },
      "source": [
        "## Teacher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hr6SLGPeKRI-"
      },
      "outputs": [],
      "source": [
        "# Create the teacher\n",
        "teacher = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "        layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"teacher\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student"
      ],
      "metadata": {
        "id": "tOk-4hSWTBOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the student\n",
        "student = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"student\",\n",
        ")\n",
        "\n",
        "# Clone student for later comparison\n",
        "student_scratch = keras.models.clone_model(student)"
      ],
      "metadata": {
        "id": "RfDFZsCxTBYJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0YpYzDfKRI_"
      },
      "source": [
        "## Train the teacher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zQo1XqZdKRJA",
        "outputId": "7322fee3-00f1-4a56-91d4-b1be903979d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 20s 9ms/step - loss: 0.1518 - sparse_categorical_accuracy: 0.9538\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1045 - sparse_categorical_accuracy: 0.9702\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0988 - sparse_categorical_accuracy: 0.9726\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0985 - sparse_categorical_accuracy: 0.9739\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0941 - sparse_categorical_accuracy: 0.9767\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1158 - sparse_categorical_accuracy: 0.9743\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.11584722995758057, 0.9743000268936157]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "teacher.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "teacher.fit(x_train, y_train, epochs=5)\n",
        "teacher.evaluate(x_test, y_test) # Loss, Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TeOgveyKRJA"
      },
      "source": [
        "## Distill teacher to student\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iH6Uc_kCKRJA",
        "outputId": "8e82b2c5-52d8-480a-c8b8-3ffc2b1f155c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 7s 3ms/step - sparse_categorical_accuracy: 0.9171\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 6s 3ms/step - sparse_categorical_accuracy: 0.9673\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 6s 3ms/step - sparse_categorical_accuracy: 0.9757\n",
            "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9766\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9765999913215637"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "distiller = Distiller(student=student, teacher=teacher)\n",
        "distiller.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.1,\n",
        "    temperature=10,\n",
        ")\n",
        "\n",
        "distiller.fit(x_train, y_train, epochs=3)\n",
        "distiller.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CCDGsqsKRJA"
      },
      "source": [
        "## Train student from scratch for comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ALRF5J4RKRJA",
        "outputId": "177348f5-ecd8-41e0-a79e-c10038f55930",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2231 - sparse_categorical_accuracy: 0.9341\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0905 - sparse_categorical_accuracy: 0.9725\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0740 - sparse_categorical_accuracy: 0.9771\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0739 - sparse_categorical_accuracy: 0.9758\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07391741871833801, 0.9757999777793884]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Train student as doen usually\n",
        "student_scratch.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate student trained from scratch.\n",
        "student_scratch.fit(x_train, y_train, epochs=3)\n",
        "student_scratch.evaluate(x_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "knowledge_distillation",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}