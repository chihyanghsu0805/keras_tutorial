{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chihyanghsu0805/machine_learning/blob/tutorials/tutorials/keras/graph_attention_network/node_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook re-implements the tutorial on https://keras.io/examples/graph/gat_node_classification/."
      ],
      "metadata": {
        "id": "JVkUWsbsq3vH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Libraries"
      ],
      "metadata": {
        "id": "nRacE48HsOr8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i-69pxJGqmX8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", 6)\n",
        "pd.set_option(\"display.max_rows\", 6)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Dataset"
      ],
      "metadata": {
        "id": "mDNSKm_usSQQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ULPmSVa5qmX-"
      },
      "outputs": [],
      "source": [
        "zip_file = keras.utils.get_file(\n",
        "    fname=\"cora.tgz\",\n",
        "    origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\",\n",
        "    extract=True,\n",
        ")\n",
        "\n",
        "data_dir = os.path.join(os.path.dirname(zip_file), \"cora\")\n",
        "\n",
        "citations = pd.read_csv(\n",
        "    os.path.join(data_dir, \"cora.cites\"),\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=[\"target\", \"source\"],\n",
        ")\n",
        "\n",
        "papers = pd.read_csv(\n",
        "    os.path.join(data_dir, \"cora.content\"),\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=[\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "Jnejm6dqs4GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rows are papers, columns are paper_id, features, subject. \n",
        "print(papers)"
      ],
      "metadata": {
        "id": "QmbP78L6ryOy",
        "outputId": "f6d306f1-dedb-4458-f8e6-558cff85f6d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      paper_id  term_0  term_1  ...  term_1431  term_1432  \\\n",
            "0        31336       0       0  ...          0          0   \n",
            "1      1061127       0       0  ...          0          0   \n",
            "2      1106406       0       0  ...          0          0   \n",
            "...        ...     ...     ...  ...        ...        ...   \n",
            "2705   1128978       0       0  ...          0          0   \n",
            "2706    117328       0       0  ...          0          0   \n",
            "2707     24043       0       0  ...          0          0   \n",
            "\n",
            "                     subject  \n",
            "0            Neural_Networks  \n",
            "1              Rule_Learning  \n",
            "2     Reinforcement_Learning  \n",
            "...                      ...  \n",
            "2705      Genetic_Algorithms  \n",
            "2706              Case_Based  \n",
            "2707         Neural_Networks  \n",
            "\n",
            "[2708 rows x 1435 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Edges / Citations\n",
        "print(citations)"
      ],
      "metadata": {
        "id": "a0kdw4RWr1j8",
        "outputId": "78bae532-ea1b-4c55-b970-afa5f94b206d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      target   source\n",
            "0         35     1033\n",
            "1         35   103482\n",
            "2         35   103515\n",
            "...      ...      ...\n",
            "5426  853118  1140289\n",
            "5427  853155   853118\n",
            "5428  954315  1155073\n",
            "\n",
            "[5429 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_values = sorted(papers[\"subject\"].unique())\n",
        "print(class_values)"
      ],
      "metadata": {
        "id": "gH0i_gAkvyco",
        "outputId": "fb13c713-e22c-4453-89cb-79bd2e81baef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Case_Based', 'Genetic_Algorithms', 'Neural_Networks', 'Probabilistic_Methods', 'Reinforcement_Learning', 'Rule_Learning', 'Theory']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = collections.Counter(papers[\"subject\"])\n",
        "plt.figure(figsize = (20, 5))\n",
        "plt.bar(class_counts.keys(), class_counts.values())"
      ],
      "metadata": {
        "id": "Fy_oA6Ny6-9A",
        "outputId": "a63f9bb6-f4c7-402a-f7be-46c7da2ca07f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 7 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map entries to index\n",
        "class_idx = {name: id for id, name in enumerate(class_values)}\n",
        "paper_idx = {\n",
        "    name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))\n",
        "}\n",
        "\n",
        "print(class_idx)\n",
        "print(paper_idx)"
      ],
      "metadata": {
        "id": "hIxfJM0uwAnx",
        "outputId": "4689e697-3513-44e1-d9fd-c51714641768",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Case_Based': 0, 'Genetic_Algorithms': 1, 'Neural_Networks': 2, 'Probabilistic_Methods': 3, 'Reinforcement_Learning': 4, 'Rule_Learning': 5, 'Theory': 6}\n",
            "{35: 0, 40: 1, 114: 2, 117: 3, 128: 4, 130: 5, 164: 6, 288: 7, 424: 8, 434: 9, 463: 10, 504: 11, 506: 12, 887: 13, 906: 14, 910: 15, 936: 16, 940: 17, 941: 18, 943: 19, 1026: 20, 1033: 21, 1034: 22, 1035: 23, 1213: 24, 1237: 25, 1246: 26, 1272: 27, 1365: 28, 1385: 29, 1481: 30, 1688: 31, 1694: 32, 1717: 33, 1786: 34, 1817: 35, 1919: 36, 1949: 37, 1951: 38, 1952: 39, 1953: 40, 1955: 41, 1956: 42, 1959: 43, 1997: 44, 1999: 45, 2354: 46, 2440: 47, 2653: 48, 2654: 49, 2658: 50, 2663: 51, 2665: 52, 2695: 53, 2696: 54, 2698: 55, 2702: 56, 3084: 57, 3085: 58, 3095: 59, 3097: 60, 3101: 61, 3112: 62, 3187: 63, 3191: 64, 3192: 65, 3217: 66, 3218: 67, 3220: 68, 3222: 69, 3223: 70, 3229: 71, 3231: 72, 3232: 73, 3233: 74, 3235: 75, 3236: 76, 3237: 77, 3240: 78, 3243: 79, 3828: 80, 3932: 81, 4274: 82, 4329: 83, 4330: 84, 4335: 85, 4553: 86, 4584: 87, 4637: 88, 4649: 89, 4660: 90, 4804: 91, 4878: 92, 4983: 93, 5038: 94, 5055: 95, 5062: 96, 5064: 97, 5069: 98, 5075: 99, 5086: 100, 5194: 101, 5348: 102, 5454: 103, 5462: 104, 5600: 105, 5869: 106, 5959: 107, 5966: 108, 6125: 109, 6130: 110, 6151: 111, 6152: 112, 6155: 113, 6163: 114, 6169: 115, 6170: 116, 6184: 117, 6196: 118, 6209: 119, 6210: 120, 6213: 121, 6214: 122, 6215: 123, 6216: 124, 6217: 125, 6220: 126, 6224: 127, 6238: 128, 6311: 129, 6318: 130, 6334: 131, 6343: 132, 6344: 133, 6346: 134, 6378: 135, 6385: 136, 6539: 137, 6639: 138, 6741: 139, 6767: 140, 6771: 141, 6775: 142, 6782: 143, 6784: 144, 6786: 145, 6814: 146, 6818: 147, 6898: 148, 6910: 149, 6913: 150, 6917: 151, 6923: 152, 6925: 153, 6935: 154, 6939: 155, 6941: 156, 7022: 157, 7032: 158, 7041: 159, 7047: 160, 7272: 161, 7276: 162, 7296: 163, 7297: 164, 7419: 165, 7430: 166, 7432: 167, 7532: 168, 7537: 169, 7867: 170, 8079: 171, 8213: 172, 8224: 173, 8581: 174, 8591: 175, 8594: 176, 8617: 177, 8619: 178, 8687: 179, 8696: 180, 8699: 181, 8703: 182, 8766: 183, 8821: 184, 8832: 185, 8865: 186, 8872: 187, 8874: 188, 8875: 189, 8961: 190, 9513: 191, 9515: 192, 9559: 193, 9581: 194, 9586: 195, 9708: 196, 9716: 197, 10169: 198, 10174: 199, 10177: 200, 10183: 201, 10186: 202, 10430: 203, 10435: 204, 10531: 205, 10793: 206, 10796: 207, 10798: 208, 10981: 209, 11093: 210, 11148: 211, 11325: 212, 11326: 213, 11335: 214, 11337: 215, 11339: 216, 11342: 217, 12155: 218, 12158: 219, 12165: 220, 12169: 221, 12182: 222, 12194: 223, 12195: 224, 12197: 225, 12198: 226, 12199: 227, 12210: 228, 12211: 229, 12238: 230, 12247: 231, 12275: 232, 12330: 233, 12337: 234, 12347: 235, 12350: 236, 12359: 237, 12439: 238, 12558: 239, 12576: 240, 12631: 241, 12638: 242, 12946: 243, 12960: 244, 13024: 245, 13136: 246, 13193: 247, 13195: 248, 13205: 249, 13208: 250, 13212: 251, 13213: 252, 13269: 253, 13652: 254, 13654: 255, 13656: 256, 13658: 257, 13686: 258, 13717: 259, 13885: 260, 13917: 261, 13960: 262, 13966: 263, 13972: 264, 13982: 265, 14062: 266, 14083: 267, 14090: 268, 14428: 269, 14429: 270, 14430: 271, 14431: 272, 14529: 273, 14531: 274, 14545: 275, 14549: 276, 14807: 277, 15076: 278, 15429: 279, 15431: 280, 15670: 281, 15889: 282, 15892: 283, 15984: 284, 15987: 285, 16008: 286, 16437: 287, 16451: 288, 16461: 289, 16470: 290, 16471: 291, 16474: 292, 16476: 293, 16485: 294, 16819: 295, 16843: 296, 17201: 297, 17208: 298, 17242: 299, 17363: 300, 17476: 301, 17477: 302, 17488: 303, 17798: 304, 17811: 305, 17821: 306, 18251: 307, 18313: 308, 18532: 309, 18536: 310, 18582: 311, 18615: 312, 18619: 313, 18770: 314, 18773: 315, 18774: 316, 18777: 317, 18781: 318, 18785: 319, 18811: 320, 18812: 321, 18815: 322, 18832: 323, 18833: 324, 18834: 325, 19045: 326, 19231: 327, 19621: 328, 19697: 329, 20178: 330, 20179: 331, 20180: 332, 20193: 333, 20526: 334, 20528: 335, 20534: 336, 20584: 337, 20592: 338, 20593: 339, 20601: 340, 20602: 341, 20821: 342, 20833: 343, 20850: 344, 20857: 345, 20920: 346, 20923: 347, 20924: 348, 20942: 349, 20972: 350, 22229: 351, 22241: 352, 22386: 353, 22431: 354, 22563: 355, 22564: 356, 22566: 357, 22835: 358, 22869: 359, 22874: 360, 22875: 361, 22876: 362, 22883: 363, 22886: 364, 23069: 365, 23070: 366, 23116: 367, 23258: 368, 23448: 369, 23502: 370, 23507: 371, 23545: 372, 23546: 373, 23738: 374, 23774: 375, 24043: 376, 24476: 377, 24530: 378, 24966: 379, 24974: 380, 25181: 381, 25184: 382, 25413: 383, 25702: 384, 25772: 385, 25791: 386, 25794: 387, 25805: 388, 26850: 389, 27174: 390, 27199: 391, 27203: 392, 27230: 393, 27241: 394, 27243: 395, 27246: 396, 27249: 397, 27250: 398, 27510: 399, 27514: 400, 27530: 401, 27531: 402, 27535: 403, 27543: 404, 27606: 405, 27612: 406, 27623: 407, 27627: 408, 27631: 409, 27632: 410, 27895: 411, 28026: 412, 28202: 413, 28227: 414, 28230: 415, 28249: 416, 28254: 417, 28265: 418, 28267: 419, 28278: 420, 28287: 421, 28290: 422, 28336: 423, 28350: 424, 28359: 425, 28385: 426, 28387: 427, 28389: 428, 28412: 429, 28447: 430, 28456: 431, 28471: 432, 28473: 433, 28485: 434, 28487: 435, 28489: 436, 28491: 437, 28504: 438, 28542: 439, 28632: 440, 28640: 441, 28641: 442, 28649: 443, 28674: 444, 28851: 445, 28957: 446, 28964: 447, 29492: 448, 29708: 449, 29723: 450, 29738: 451, 30817: 452, 30895: 453, 30901: 454, 30934: 455, 30973: 456, 31043: 457, 31055: 458, 31083: 459, 31097: 460, 31105: 461, 31336: 462, 31349: 463, 31353: 464, 31479: 465, 31483: 466, 31489: 467, 31769: 468, 31863: 469, 31927: 470, 31932: 471, 32083: 472, 32260: 473, 32276: 474, 32688: 475, 32698: 476, 32872: 477, 33013: 478, 33231: 479, 33301: 480, 33303: 481, 33325: 482, 33412: 483, 33818: 484, 33823: 485, 33895: 486, 33904: 487, 33907: 488, 34082: 489, 34257: 490, 34263: 491, 34266: 492, 34315: 493, 34355: 494, 34708: 495, 34961: 496, 34979: 497, 35061: 498, 35070: 499, 35335: 500, 35343: 501, 35490: 502, 35778: 503, 35797: 504, 35852: 505, 35854: 506, 35863: 507, 35905: 508, 35922: 509, 36131: 510, 36140: 511, 36145: 512, 36162: 513, 36167: 514, 36620: 515, 36802: 516, 37483: 517, 37541: 518, 37879: 519, 37884: 520, 37888: 521, 37998: 522, 38000: 523, 38205: 524, 38480: 525, 38537: 526, 38722: 527, 38771: 528, 38829: 529, 38839: 530, 38845: 531, 38846: 532, 39124: 533, 39126: 534, 39127: 535, 39130: 536, 39131: 537, 39165: 538, 39199: 539, 39210: 540, 39403: 541, 39474: 542, 39890: 543, 39904: 544, 40124: 545, 40125: 546, 40131: 547, 40135: 548, 40151: 549, 40583: 550, 40605: 551, 40886: 552, 40922: 553, 41216: 554, 41417: 555, 41666: 556, 41714: 557, 41732: 558, 42156: 559, 42207: 560, 42209: 561, 42221: 562, 42847: 563, 42848: 564, 43165: 565, 43186: 566, 43639: 567, 43698: 568, 44017: 569, 44121: 570, 44368: 571, 44455: 572, 44514: 573, 45052: 574, 45061: 575, 45188: 576, 45189: 577, 45212: 578, 45533: 579, 45599: 580, 45603: 581, 45605: 582, 46079: 583, 46431: 584, 46452: 585, 46468: 586, 46470: 587, 46476: 588, 46491: 589, 46500: 590, 46501: 591, 46536: 592, 46547: 593, 46887: 594, 47570: 595, 47682: 596, 47683: 597, 47684: 598, 47839: 599, 48066: 600, 48075: 601, 48550: 602, 48555: 603, 48764: 604, 48766: 605, 48768: 606, 48781: 607, 49482: 608, 49660: 609, 49720: 610, 49753: 611, 49811: 612, 49843: 613, 49844: 614, 49847: 615, 49895: 616, 50336: 617, 50337: 618, 50354: 619, 50381: 620, 50807: 621, 50838: 622, 50980: 623, 51045: 624, 51049: 625, 51052: 626, 51180: 627, 51831: 628, 51834: 629, 51866: 630, 51879: 631, 51909: 632, 51934: 633, 52000: 634, 52003: 635, 52007: 636, 52515: 637, 52784: 638, 52835: 639, 52847: 640, 53942: 641, 54129: 642, 54131: 643, 54132: 644, 54550: 645, 54844: 646, 55403: 647, 55770: 648, 55801: 649, 55968: 650, 56112: 651, 56115: 652, 56119: 653, 56167: 654, 56708: 655, 56709: 656, 57119: 657, 57764: 658, 57773: 659, 57922: 660, 57932: 661, 57948: 662, 58268: 663, 58436: 664, 58453: 665, 58454: 666, 58540: 667, 58552: 668, 58758: 669, 59045: 670, 59244: 671, 59626: 672, 59715: 673, 59772: 674, 59798: 675, 60159: 676, 60169: 677, 60170: 678, 60560: 679, 60682: 680, 61069: 681, 61073: 682, 61312: 683, 61417: 684, 62274: 685, 62329: 686, 62333: 687, 62347: 688, 62389: 689, 62417: 690, 62607: 691, 62634: 692, 62676: 693, 62718: 694, 63477: 695, 63486: 696, 63549: 697, 63812: 698, 63832: 699, 63835: 700, 63915: 701, 63931: 702, 64271: 703, 64319: 704, 64484: 705, 64519: 706, 65057: 707, 65074: 708, 65212: 709, 65650: 710, 65653: 711, 66556: 712, 66563: 713, 66564: 714, 66594: 715, 66596: 716, 66751: 717, 66782: 718, 66794: 719, 66805: 720, 66809: 721, 66982: 722, 66986: 723, 66990: 724, 67245: 725, 67246: 726, 67292: 727, 67415: 728, 67584: 729, 67633: 730, 68115: 731, 68224: 732, 68463: 733, 68495: 734, 68505: 735, 69198: 736, 69284: 737, 69296: 738, 69392: 739, 69397: 740, 69418: 741, 70281: 742, 70441: 743, 70442: 744, 70444: 745, 70520: 746, 70970: 747, 71336: 748, 71736: 749, 71904: 750, 72056: 751, 72101: 752, 72406: 753, 72805: 754, 72908: 755, 73119: 756, 73146: 757, 73162: 758, 73323: 759, 73327: 760, 73712: 761, 73972: 762, 74427: 763, 74698: 764, 74700: 765, 74749: 766, 74821: 767, 74920: 768, 74921: 769, 74937: 770, 74975: 771, 75121: 772, 75318: 773, 75674: 774, 75691: 775, 75693: 776, 75694: 777, 75695: 778, 75969: 779, 75972: 780, 75983: 781, 77108: 782, 77112: 783, 77438: 784, 77515: 785, 77758: 786, 77826: 787, 77829: 788, 78508: 789, 78511: 790, 78549: 791, 78552: 792, 78555: 793, 78557: 794, 78994: 795, 79809: 796, 79817: 797, 80491: 798, 80515: 799, 80656: 800, 81350: 801, 81714: 802, 81722: 803, 82087: 804, 82090: 805, 82098: 806, 82664: 807, 82666: 808, 82920: 809, 83449: 810, 83461: 811, 83725: 812, 83746: 813, 83826: 814, 83847: 815, 84020: 816, 84021: 817, 84459: 818, 84695: 819, 85299: 820, 85324: 821, 85352: 822, 85449: 823, 85452: 824, 85688: 825, 86258: 826, 86359: 827, 86840: 828, 86923: 829, 87363: 830, 87417: 831, 87482: 832, 87915: 833, 88356: 834, 89308: 835, 89335: 836, 89416: 837, 89547: 838, 90470: 839, 90655: 840, 90888: 841, 91038: 842, 91581: 843, 91852: 844, 91853: 845, 91975: 846, 92065: 847, 92589: 848, 93273: 849, 93318: 850, 93320: 851, 93555: 852, 93755: 853, 93923: 854, 94229: 855, 94416: 856, 94639: 857, 94641: 858, 94713: 859, 94953: 860, 95188: 861, 95198: 862, 95225: 863, 95435: 864, 95579: 865, 95586: 866, 95588: 867, 95589: 868, 95594: 869, 95597: 870, 95642: 871, 95718: 872, 95719: 873, 96335: 874, 96845: 875, 96847: 876, 96851: 877, 97377: 878, 97390: 879, 97645: 880, 97892: 881, 98693: 882, 98698: 883, 99023: 884, 99025: 885, 99030: 886, 100197: 887, 100701: 888, 100935: 889, 100961: 890, 101143: 891, 101145: 892, 101261: 893, 101263: 894, 101660: 895, 101662: 896, 101811: 897, 102061: 898, 102406: 899, 102879: 900, 102884: 901, 102938: 902, 102939: 903, 103430: 904, 103482: 905, 103515: 906, 103528: 907, 103529: 908, 103531: 909, 103537: 910, 103543: 911, 104840: 912, 105057: 913, 105856: 914, 105865: 915, 105899: 916, 106590: 917, 107177: 918, 107251: 919, 107252: 920, 107569: 921, 108047: 922, 108962: 923, 108963: 924, 108974: 925, 108983: 926, 109323: 927, 110041: 928, 110162: 929, 110163: 930, 110164: 931, 111676: 932, 111770: 933, 111866: 934, 112099: 935, 112378: 936, 112787: 937, 112813: 938, 114189: 939, 114308: 940, 114966: 941, 115188: 942, 116021: 943, 116081: 944, 116084: 945, 116087: 946, 116512: 947, 116528: 948, 116545: 949, 116552: 950, 116553: 951, 116790: 952, 117315: 953, 117316: 954, 117328: 955, 118079: 956, 118259: 957, 118260: 958, 118424: 959, 118435: 960, 118436: 961, 118558: 962, 118559: 963, 118682: 964, 118873: 965, 119686: 966, 119712: 967, 119761: 968, 119956: 969, 120013: 970, 120039: 971, 120084: 972, 120817: 973, 121792: 974, 123556: 975, 123825: 976, 124064: 977, 124224: 978, 124296: 979, 124734: 980, 124828: 981, 124952: 982, 126128: 983, 126793: 984, 126867: 985, 126868: 986, 126909: 987, 126912: 988, 126920: 989, 126926: 990, 126927: 991, 127033: 992, 127940: 993, 128202: 994, 128203: 995, 128383: 996, 128540: 997, 129042: 998, 129045: 999, 129287: 1000, 129558: 1001, 129896: 1002, 129897: 1003, 131042: 1004, 131117: 1005, 131122: 1006, 131315: 1007, 131317: 1008, 131318: 1009, 132806: 1010, 132821: 1011, 133550: 1012, 133553: 1013, 133563: 1014, 133566: 1015, 133567: 1016, 133615: 1017, 133628: 1018, 134060: 1019, 134128: 1020, 134199: 1021, 134219: 1022, 134307: 1023, 134314: 1024, 134315: 1025, 134316: 1026, 134320: 1027, 135130: 1028, 135464: 1029, 135765: 1030, 135766: 1031, 135798: 1032, 136665: 1033, 136766: 1034, 136767: 1035, 136768: 1036, 137130: 1037, 137359: 1038, 137380: 1039, 137790: 1040, 137849: 1041, 137868: 1042, 137873: 1043, 137956: 1044, 139547: 1045, 139738: 1046, 139865: 1047, 140005: 1048, 140569: 1049, 141160: 1050, 141171: 1051, 141324: 1052, 141342: 1053, 141347: 1054, 141596: 1055, 141868: 1056, 142268: 1057, 143323: 1058, 143476: 1059, 143676: 1060, 143801: 1061, 144212: 1062, 144330: 1063, 144408: 1064, 144679: 1065, 144701: 1066, 145134: 1067, 145176: 1068, 145215: 1069, 145315: 1070, 145384: 1071, 147870: 1072, 148170: 1073, 148341: 1074, 148399: 1075, 149139: 1076, 149669: 1077, 151430: 1078, 151708: 1079, 152219: 1080, 152226: 1081, 152227: 1082, 152483: 1083, 152731: 1084, 153063: 1085, 153598: 1086, 154023: 1087, 154047: 1088, 154134: 1089, 154982: 1090, 155158: 1091, 155277: 1092, 155736: 1093, 155738: 1094, 156794: 1095, 156977: 1096, 157401: 1097, 157761: 1098, 157805: 1099, 158098: 1100, 158172: 1101, 158614: 1102, 158812: 1103, 159084: 1104, 159085: 1105, 159897: 1106, 160705: 1107, 160732: 1108, 161221: 1109, 162075: 1110, 162080: 1111, 162664: 1112, 163235: 1113, 164885: 1114, 166420: 1115, 166825: 1116, 166989: 1117, 167205: 1118, 167656: 1119, 167670: 1120, 168332: 1121, 168410: 1122, 168958: 1123, 169279: 1124, 169280: 1125, 170338: 1126, 170798: 1127, 171225: 1128, 171954: 1129, 173863: 1130, 173884: 1131, 174418: 1132, 174425: 1133, 175256: 1134, 175291: 1135, 175548: 1136, 175576: 1137, 175909: 1138, 177115: 1139, 177993: 1140, 177998: 1141, 178209: 1142, 178718: 1143, 178727: 1144, 179180: 1145, 179702: 1146, 179706: 1147, 180187: 1148, 180301: 1149, 180373: 1150, 180399: 1151, 181782: 1152, 182093: 1153, 182094: 1154, 184157: 1155, 184918: 1156, 187260: 1157, 187354: 1158, 188318: 1159, 188471: 1160, 189566: 1161, 189571: 1162, 189572: 1163, 189574: 1164, 189577: 1165, 189620: 1166, 189623: 1167, 189655: 1168, 189708: 1169, 189721: 1170, 189774: 1171, 189856: 1172, 190697: 1173, 190698: 1174, 190706: 1175, 191216: 1176, 191222: 1177, 191404: 1178, 192734: 1179, 192850: 1180, 192870: 1181, 193347: 1182, 193352: 1183, 193354: 1184, 193742: 1185, 193918: 1186, 193931: 1187, 193932: 1188, 194223: 1189, 194609: 1190, 194617: 1191, 194645: 1192, 195150: 1193, 195361: 1194, 195792: 1195, 197054: 1196, 197452: 1197, 197783: 1198, 198443: 1199, 198653: 1200, 198866: 1201, 199571: 1202, 200480: 1203, 200630: 1204, 202520: 1205, 202522: 1206, 202639: 1207, 203646: 1208, 205192: 1209, 205196: 1210, 206259: 1211, 206371: 1212, 206524: 1213, 207395: 1214, 208345: 1215, 210309: 1216, 210871: 1217, 210872: 1218, 211432: 1219, 211875: 1220, 211906: 1221, 212097: 1222, 212107: 1223, 212777: 1224, 212930: 1225, 213246: 1226, 213279: 1227, 214472: 1228, 215912: 1229, 216877: 1230, 216878: 1231, 217115: 1232, 217139: 1233, 217852: 1234, 217984: 1235, 218410: 1236, 218666: 1237, 218682: 1238, 219218: 1239, 219239: 1240, 219446: 1241, 219976: 1242, 220420: 1243, 221302: 1244, 226698: 1245, 227178: 1246, 227286: 1247, 228990: 1248, 228992: 1249, 229635: 1250, 230300: 1251, 230879: 1252, 230884: 1253, 231198: 1254, 231249: 1255, 232605: 1256, 232606: 1257, 232860: 1258, 233106: 1259, 235670: 1260, 235678: 1261, 235679: 1262, 235683: 1263, 235776: 1264, 236759: 1265, 237376: 1266, 237489: 1267, 237521: 1268, 238099: 1269, 238401: 1270, 239800: 1271, 239810: 1272, 239829: 1273, 240321: 1274, 240791: 1275, 241133: 1276, 241821: 1277, 242637: 1278, 242663: 1279, 243274: 1280, 243483: 1281, 245288: 1282, 245955: 1283, 246618: 1284, 248119: 1285, 248395: 1286, 248425: 1287, 248431: 1288, 248823: 1289, 249421: 1290, 249858: 1291, 250566: 1292, 251756: 1293, 252715: 1294, 252725: 1295, 253762: 1296, 253971: 1297, 254923: 1298, 255233: 1299, 255628: 1300, 256106: 1301, 258259: 1302, 259126: 1303, 259701: 1304, 259702: 1305, 259772: 1306, 260121: 1307, 260979: 1308, 261040: 1309, 262108: 1310, 262121: 1311, 262178: 1312, 263069: 1313, 263279: 1314, 263482: 1315, 263486: 1316, 263498: 1317, 263553: 1318, 264347: 1319, 264556: 1320, 265203: 1321, 267003: 1322, 267824: 1323, 270085: 1324, 270456: 1325, 270600: 1326, 272345: 1327, 272720: 1328, 273152: 1329, 273949: 1330, 277263: 1331, 278394: 1332, 278403: 1333, 280876: 1334, 282700: 1335, 284023: 1336, 284025: 1337, 284414: 1338, 285675: 1339, 285687: 1340, 286500: 1341, 286513: 1342, 286562: 1343, 287787: 1344, 288107: 1345, 289085: 1346, 289088: 1347, 289779: 1348, 289780: 1349, 289781: 1350, 289885: 1351, 289945: 1352, 292277: 1353, 293271: 1354, 293285: 1355, 293974: 1356, 294030: 1357, 294126: 1358, 294145: 1359, 294239: 1360, 299195: 1361, 299197: 1362, 300071: 1363, 300806: 1364, 302545: 1365, 307015: 1366, 307336: 1367, 307656: 1368, 308003: 1369, 308232: 1370, 308529: 1371, 308920: 1372, 309476: 1373, 310530: 1374, 310653: 1375, 310742: 1376, 312409: 1377, 314459: 1378, 315266: 1379, 315789: 1380, 318071: 1381, 318187: 1382, 321004: 1383, 321861: 1384, 323128: 1385, 325314: 1386, 325497: 1387, 328370: 1388, 330148: 1389, 330208: 1390, 334153: 1391, 335042: 1392, 335733: 1393, 337766: 1394, 340075: 1395, 340078: 1396, 340299: 1397, 341188: 1398, 342802: 1399, 345340: 1400, 346243: 1401, 346292: 1402, 348305: 1403, 348437: 1404, 350319: 1405, 350362: 1406, 350373: 1407, 353541: 1408, 354004: 1409, 358866: 1410, 358884: 1411, 358887: 1412, 358894: 1413, 359067: 1414, 360028: 1415, 362926: 1416, 365294: 1417, 367312: 1418, 368431: 1419, 368605: 1420, 368657: 1421, 370366: 1422, 372862: 1423, 375605: 1424, 375825: 1425, 376704: 1426, 377303: 1427, 379288: 1428, 380341: 1429, 384428: 1430, 385067: 1431, 385251: 1432, 385572: 1433, 387795: 1434, 389715: 1435, 390693: 1436, 390889: 1437, 390894: 1438, 390896: 1439, 390922: 1440, 395075: 1441, 395540: 1442, 395547: 1443, 395553: 1444, 395725: 1445, 396412: 1446, 397488: 1447, 397590: 1448, 399173: 1449, 399339: 1450, 399370: 1451, 400356: 1452, 400455: 1453, 400473: 1454, 408885: 1455, 409255: 1456, 409725: 1457, 411005: 1458, 411092: 1459, 415693: 1460, 416455: 1461, 416867: 1462, 416964: 1463, 417017: 1464, 421481: 1465, 423463: 1466, 423816: 1467, 424540: 1468, 427606: 1469, 428610: 1470, 429781: 1471, 429805: 1472, 430329: 1473, 430574: 1474, 430711: 1475, 431206: 1476, 436796: 1477, 440815: 1478, 444191: 1479, 444240: 1480, 445938: 1481, 446271: 1482, 446610: 1483, 447224: 1484, 447250: 1485, 449841: 1486, 458439: 1487, 459206: 1488, 459213: 1489, 459214: 1490, 459216: 1491, 463825: 1492, 466170: 1493, 467383: 1494, 469504: 1495, 470511: 1496, 481073: 1497, 486840: 1498, 502574: 1499, 503871: 1500, 503877: 1501, 503883: 1502, 503893: 1503, 509233: 1504, 509315: 1505, 509379: 1506, 510715: 1507, 510718: 1508, 513189: 1509, 519318: 1510, 519353: 1511, 520471: 1512, 521183: 1513, 521207: 1514, 521251: 1515, 521252: 1516, 521269: 1517, 521855: 1518, 522338: 1519, 523010: 1520, 523394: 1521, 523574: 1522, 529165: 1523, 531348: 1524, 531351: 1525, 545647: 1526, 552469: 1527, 559804: 1528, 560936: 1529, 561238: 1530, 561364: 1531, 561568: 1532, 561581: 1533, 561582: 1534, 561593: 1535, 561595: 1536, 561610: 1537, 561611: 1538, 561613: 1539, 561674: 1540, 561789: 1541, 561809: 1542, 562067: 1543, 562123: 1544, 562940: 1545, 563613: 1546, 566488: 1547, 566653: 1548, 566664: 1549, 567005: 1550, 567018: 1551, 568045: 1552, 568857: 1553, 573535: 1554, 573553: 1555, 573964: 1556, 573978: 1557, 574009: 1558, 574264: 1559, 574462: 1560, 574710: 1561, 575077: 1562, 575292: 1563, 575331: 1564, 575402: 1565, 575795: 1566, 576257: 1567, 576362: 1568, 576691: 1569, 576725: 1570, 576795: 1571, 576973: 1572, 577086: 1573, 577227: 1574, 577331: 1575, 578306: 1576, 578309: 1577, 578337: 1578, 578347: 1579, 578365: 1580, 578645: 1581, 578646: 1582, 578649: 1583, 578650: 1584, 578669: 1585, 578780: 1586, 578845: 1587, 578898: 1588, 579008: 1589, 579108: 1590, 582139: 1591, 582343: 1592, 582349: 1593, 582511: 1594, 583318: 1595, 589923: 1596, 590022: 1597, 591016: 1598, 591017: 1599, 592826: 1600, 592830: 1601, 592973: 1602, 592975: 1603, 592986: 1604, 592993: 1605, 592996: 1606, 593022: 1607, 593060: 1608, 593068: 1609, 593091: 1610, 593104: 1611, 593105: 1612, 593155: 1613, 593201: 1614, 593209: 1615, 593210: 1616, 593240: 1617, 593248: 1618, 593260: 1619, 593328: 1620, 593329: 1621, 593544: 1622, 593559: 1623, 593560: 1624, 593813: 1625, 593859: 1626, 593921: 1627, 593942: 1628, 594011: 1629, 594025: 1630, 594039: 1631, 594047: 1632, 594119: 1633, 594387: 1634, 594483: 1635, 594511: 1636, 594543: 1637, 594649: 1638, 594900: 1639, 595056: 1640, 595063: 1641, 595157: 1642, 595193: 1643, 596075: 1644, 601462: 1645, 601561: 1646, 601567: 1647, 604073: 1648, 606479: 1649, 606647: 1650, 608190: 1651, 608191: 1652, 608292: 1653, 608326: 1654, 610529: 1655, 612306: 1656, 613409: 1657, 616336: 1658, 617378: 1659, 617575: 1660, 621555: 1661, 626530: 1662, 626531: 1663, 626574: 1664, 626999: 1665, 627024: 1666, 628458: 1667, 628459: 1668, 628500: 1669, 628667: 1670, 628668: 1671, 628751: 1672, 628764: 1673, 628766: 1674, 628815: 1675, 628888: 1676, 630817: 1677, 630890: 1678, 631015: 1679, 631052: 1680, 632796: 1681, 632874: 1682, 632935: 1683, 633030: 1684, 633031: 1685, 633081: 1686, 633585: 1687, 633721: 1688, 634902: 1689, 634904: 1690, 634938: 1691, 634975: 1692, 636098: 1693, 636500: 1694, 636511: 1695, 640617: 1696, 641956: 1697, 641976: 1698, 642593: 1699, 642621: 1700, 642641: 1701, 642681: 1702, 642798: 1703, 642827: 1704, 642847: 1705, 642894: 1706, 642920: 1707, 642930: 1708, 643003: 1709, 643069: 1710, 643199: 1711, 643221: 1712, 643239: 1713, 643485: 1714, 643597: 1715, 643695: 1716, 643734: 1717, 643735: 1718, 643777: 1719, 644093: 1720, 644334: 1721, 644361: 1722, 644363: 1723, 644427: 1724, 644441: 1725, 644448: 1726, 644470: 1727, 644494: 1728, 644577: 1729, 644843: 1730, 645016: 1731, 645046: 1732, 645084: 1733, 645088: 1734, 645452: 1735, 645571: 1736, 645870: 1737, 645897: 1738, 646195: 1739, 646286: 1740, 646289: 1741, 646334: 1742, 646357: 1743, 646412: 1744, 646440: 1745, 646809: 1746, 646836: 1747, 646837: 1748, 646900: 1749, 646913: 1750, 647315: 1751, 647408: 1752, 647413: 1753, 647447: 1754, 648106: 1755, 648112: 1756, 648121: 1757, 648232: 1758, 648369: 1759, 649730: 1760, 649731: 1761, 649739: 1762, 649944: 1763, 650807: 1764, 650814: 1765, 650834: 1766, 653441: 1767, 653628: 1768, 654177: 1769, 654326: 1770, 654339: 1771, 654519: 1772, 656048: 1773, 656231: 1774, 662250: 1775, 662279: 1776, 662416: 1777, 662572: 1778, 671052: 1779, 671269: 1780, 671293: 1781, 672064: 1782, 672070: 1783, 672071: 1784, 675649: 1785, 675756: 1786, 675847: 1787, 682508: 1788, 682666: 1789, 682815: 1790, 683294: 1791, 683355: 1792, 683360: 1793, 683404: 1794, 684372: 1795, 684531: 1796, 684972: 1797, 684986: 1798, 686015: 1799, 686030: 1800, 686061: 1801, 686532: 1802, 686559: 1803, 687401: 1804, 688361: 1805, 688824: 1806, 688849: 1807, 689152: 1808, 689439: 1809, 693143: 1810, 694759: 1811, 695284: 1812, 696342: 1813, 696343: 1814, 696345: 1815, 696346: 1816, 703953: 1817, 708945: 1818, 709113: 1819, 709518: 1820, 711527: 1821, 711598: 1822, 711994: 1823, 714208: 1824, 714256: 1825, 714260: 1826, 714289: 1827, 714748: 1828, 714879: 1829, 714975: 1830, 733167: 1831, 733534: 1832, 733576: 1833, 734406: 1834, 735303: 1835, 735311: 1836, 737204: 1837, 738941: 1838, 739280: 1839, 739707: 1840, 739816: 1841, 746058: 1842, 751408: 1843, 752684: 1844, 753047: 1845, 753070: 1846, 753264: 1847, 753265: 1848, 754594: 1849, 755082: 1850, 755217: 1851, 756061: 1852, 762980: 1853, 763009: 1854, 763010: 1855, 763181: 1856, 767763: 1857, 779960: 1858, 782486: 1859, 785678: 1860, 787016: 1861, 801170: 1862, 814836: 1863, 815073: 1864, 815096: 1865, 817774: 1866, 820661: 1867, 820662: 1868, 824245: 1869, 851968: 1870, 853114: 1871, 853115: 1872, 853116: 1873, 853118: 1874, 853150: 1875, 853155: 1876, 854434: 1877, 884094: 1878, 892139: 1879, 899085: 1880, 899119: 1881, 907845: 1882, 911198: 1883, 917493: 1884, 919885: 1885, 928873: 1886, 943087: 1887, 948147: 1888, 948299: 1889, 948846: 1890, 949217: 1891, 949318: 1892, 949511: 1893, 950052: 1894, 950305: 1895, 950986: 1896, 954315: 1897, 964248: 1898, 975567: 1899, 976284: 1900, 976334: 1901, 987188: 1902, 987197: 1903, 989397: 1904, 990075: 1905, 1000012: 1906, 1022969: 1907, 1031453: 1908, 1050679: 1909, 1059953: 1910, 1061127: 1911, 1063773: 1912, 1071981: 1913, 1095507: 1914, 1102364: 1915, 1102400: 1916, 1102407: 1917, 1102442: 1918, 1102548: 1919, 1102550: 1920, 1102567: 1921, 1102625: 1922, 1102646: 1923, 1102751: 1924, 1102761: 1925, 1102794: 1926, 1102850: 1927, 1102873: 1928, 1103016: 1929, 1103031: 1930, 1103038: 1931, 1103162: 1932, 1103315: 1933, 1103383: 1934, 1103394: 1935, 1103499: 1936, 1103610: 1937, 1103676: 1938, 1103737: 1939, 1103960: 1940, 1103969: 1941, 1103979: 1942, 1103985: 1943, 1104007: 1944, 1104031: 1945, 1104055: 1946, 1104182: 1947, 1104191: 1948, 1104258: 1949, 1104261: 1950, 1104300: 1951, 1104379: 1952, 1104435: 1953, 1104449: 1954, 1104495: 1955, 1104647: 1956, 1104749: 1957, 1104769: 1958, 1104787: 1959, 1104809: 1960, 1104851: 1961, 1104946: 1962, 1104999: 1963, 1105011: 1964, 1105033: 1965, 1105062: 1966, 1105116: 1967, 1105148: 1968, 1105221: 1969, 1105231: 1970, 1105344: 1971, 1105360: 1972, 1105394: 1973, 1105428: 1974, 1105433: 1975, 1105450: 1976, 1105505: 1977, 1105530: 1978, 1105531: 1979, 1105574: 1980, 1105603: 1981, 1105622: 1982, 1105672: 1983, 1105698: 1984, 1105718: 1985, 1105764: 1986, 1105810: 1987, 1105877: 1988, 1105887: 1989, 1105932: 1990, 1106052: 1991, 1106103: 1992, 1106112: 1993, 1106172: 1994, 1106236: 1995, 1106287: 1996, 1106298: 1997, 1106330: 1998, 1106370: 1999, 1106388: 2000, 1106401: 2001, 1106406: 2002, 1106418: 2003, 1106492: 2004, 1106546: 2005, 1106547: 2006, 1106568: 2007, 1106630: 2008, 1106671: 2009, 1106764: 2010, 1106771: 2011, 1106789: 2012, 1106849: 2013, 1106854: 2014, 1106966: 2015, 1107010: 2016, 1107041: 2017, 1107062: 2018, 1107067: 2019, 1107095: 2020, 1107136: 2021, 1107140: 2022, 1107171: 2023, 1107215: 2024, 1107312: 2025, 1107319: 2026, 1107325: 2027, 1107355: 2028, 1107367: 2029, 1107385: 2030, 1107418: 2031, 1107455: 2032, 1107558: 2033, 1107567: 2034, 1107572: 2035, 1107674: 2036, 1107728: 2037, 1107808: 2038, 1107861: 2039, 1108050: 2040, 1108167: 2041, 1108169: 2042, 1108175: 2043, 1108209: 2044, 1108258: 2045, 1108267: 2046, 1108329: 2047, 1108363: 2048, 1108389: 2049, 1108551: 2050, 1108570: 2051, 1108597: 2052, 1108656: 2053, 1108728: 2054, 1108834: 2055, 1108841: 2056, 1109017: 2057, 1109185: 2058, 1109199: 2059, 1109208: 2060, 1109392: 2061, 1109439: 2062, 1109542: 2063, 1109566: 2064, 1109581: 2065, 1109830: 2066, 1109873: 2067, 1109891: 2068, 1109957: 2069, 1110000: 2070, 1110024: 2071, 1110028: 2072, 1110209: 2073, 1110256: 2074, 1110390: 2075, 1110426: 2076, 1110438: 2077, 1110494: 2078, 1110515: 2079, 1110520: 2080, 1110531: 2081, 1110546: 2082, 1110563: 2083, 1110579: 2084, 1110628: 2085, 1110768: 2086, 1110947: 2087, 1110950: 2088, 1110998: 2089, 1111052: 2090, 1111186: 2091, 1111230: 2092, 1111240: 2093, 1111265: 2094, 1111304: 2095, 1111614: 2096, 1111733: 2097, 1111788: 2098, 1111899: 2099, 1111978: 2100, 1112026: 2101, 1112071: 2102, 1112075: 2103, 1112099: 2104, 1112106: 2105, 1112194: 2106, 1112319: 2107, 1112369: 2108, 1112417: 2109, 1112426: 2110, 1112574: 2111, 1112650: 2112, 1112665: 2113, 1112686: 2114, 1112723: 2115, 1112767: 2116, 1112911: 2117, 1112929: 2118, 1113035: 2119, 1113084: 2120, 1113182: 2121, 1113438: 2122, 1113459: 2123, 1113534: 2124, 1113541: 2125, 1113551: 2126, 1113614: 2127, 1113739: 2128, 1113742: 2129, 1113828: 2130, 1113831: 2131, 1113852: 2132, 1113926: 2133, 1113934: 2134, 1113995: 2135, 1114118: 2136, 1114125: 2137, 1114153: 2138, 1114184: 2139, 1114192: 2140, 1114222: 2141, 1114239: 2142, 1114331: 2143, 1114336: 2144, 1114352: 2145, 1114364: 2146, 1114388: 2147, 1114398: 2148, 1114442: 2149, 1114502: 2150, 1114512: 2151, 1114526: 2152, 1114605: 2153, 1114629: 2154, 1114664: 2155, 1114777: 2156, 1114838: 2157, 1114864: 2158, 1114992: 2159, 1115166: 2160, 1115291: 2161, 1115375: 2162, 1115456: 2163, 1115471: 2164, 1115670: 2165, 1115677: 2166, 1115701: 2167, 1115790: 2168, 1115886: 2169, 1115959: 2170, 1116044: 2171, 1116146: 2172, 1116181: 2173, 1116268: 2174, 1116328: 2175, 1116336: 2176, 1116347: 2177, 1116397: 2178, 1116410: 2179, 1116530: 2180, 1116569: 2181, 1116594: 2182, 1116629: 2183, 1116835: 2184, 1116839: 2185, 1116842: 2186, 1116922: 2187, 1116974: 2188, 1117049: 2189, 1117089: 2190, 1117184: 2191, 1117219: 2192, 1117249: 2193, 1117348: 2194, 1117476: 2195, 1117501: 2196, 1117618: 2197, 1117653: 2198, 1117760: 2199, 1117786: 2200, 1117833: 2201, 1117920: 2202, 1117942: 2203, 1118017: 2204, 1118083: 2205, 1118092: 2206, 1118120: 2207, 1118209: 2208, 1118245: 2209, 1118286: 2210, 1118302: 2211, 1118332: 2212, 1118347: 2213, 1118388: 2214, 1118546: 2215, 1118658: 2216, 1118764: 2217, 1118823: 2218, 1118848: 2219, 1119004: 2220, 1119078: 2221, 1119140: 2222, 1119178: 2223, 1119180: 2224, 1119211: 2225, 1119216: 2226, 1119295: 2227, 1119471: 2228, 1119505: 2229, 1119623: 2230, 1119654: 2231, 1119671: 2232, 1119708: 2233, 1119742: 2234, 1119751: 2235, 1119987: 2236, 1120019: 2237, 1120020: 2238, 1120049: 2239, 1120059: 2240, 1120084: 2241, 1120138: 2242, 1120169: 2243, 1120170: 2244, 1120197: 2245, 1120211: 2246, 1120252: 2247, 1120431: 2248, 1120444: 2249, 1120563: 2250, 1120643: 2251, 1120650: 2252, 1120713: 2253, 1120731: 2254, 1120777: 2255, 1120786: 2256, 1120858: 2257, 1120866: 2258, 1120880: 2259, 1120962: 2260, 1121057: 2261, 1121063: 2262, 1121176: 2263, 1121254: 2264, 1121313: 2265, 1121398: 2266, 1121459: 2267, 1121537: 2268, 1121569: 2269, 1121603: 2270, 1121659: 2271, 1121739: 2272, 1121867: 2273, 1122304: 2274, 1122425: 2275, 1122460: 2276, 1122574: 2277, 1122580: 2278, 1122642: 2279, 1122704: 2280, 1123068: 2281, 1123087: 2282, 1123093: 2283, 1123188: 2284, 1123215: 2285, 1123239: 2286, 1123493: 2287, 1123530: 2288, 1123553: 2289, 1123576: 2290, 1123689: 2291, 1123756: 2292, 1123867: 2293, 1123926: 2294, 1123991: 2295, 1124837: 2296, 1124844: 2297, 1125082: 2298, 1125092: 2299, 1125258: 2300, 1125386: 2301, 1125393: 2302, 1125402: 2303, 1125467: 2304, 1125469: 2305, 1125492: 2306, 1125597: 2307, 1125895: 2308, 1125906: 2309, 1125909: 2310, 1125944: 2311, 1125953: 2312, 1125992: 2313, 1125993: 2314, 1126011: 2315, 1126012: 2316, 1126029: 2317, 1126037: 2318, 1126044: 2319, 1126050: 2320, 1126315: 2321, 1126350: 2322, 1126503: 2323, 1127430: 2324, 1127530: 2325, 1127541: 2326, 1127551: 2327, 1127558: 2328, 1127566: 2329, 1127619: 2330, 1127657: 2331, 1127810: 2332, 1127812: 2333, 1127851: 2334, 1127863: 2335, 1127913: 2336, 1128151: 2337, 1128198: 2338, 1128201: 2339, 1128204: 2340, 1128208: 2341, 1128227: 2342, 1128256: 2343, 1128267: 2344, 1128291: 2345, 1128314: 2346, 1128319: 2347, 1128369: 2348, 1128407: 2349, 1128425: 2350, 1128430: 2351, 1128437: 2352, 1128453: 2353, 1128531: 2354, 1128536: 2355, 1128542: 2356, 1128839: 2357, 1128846: 2358, 1128853: 2359, 1128856: 2360, 1128868: 2361, 1128881: 2362, 1128927: 2363, 1128935: 2364, 1128943: 2365, 1128945: 2366, 1128946: 2367, 1128959: 2368, 1128974: 2369, 1128975: 2370, 1128977: 2371, 1128978: 2372, 1128982: 2373, 1128985: 2374, 1128990: 2375, 1128997: 2376, 1129015: 2377, 1129018: 2378, 1129021: 2379, 1129027: 2380, 1129040: 2381, 1129096: 2382, 1129106: 2383, 1129111: 2384, 1129208: 2385, 1129243: 2386, 1129367: 2387, 1129368: 2388, 1129369: 2389, 1129442: 2390, 1129443: 2391, 1129494: 2392, 1129518: 2393, 1129570: 2394, 1129572: 2395, 1129573: 2396, 1129608: 2397, 1129610: 2398, 1129621: 2399, 1129629: 2400, 1129683: 2401, 1129778: 2402, 1129798: 2403, 1129835: 2404, 1129907: 2405, 1129994: 2406, 1130069: 2407, 1130080: 2408, 1130243: 2409, 1130356: 2410, 1130454: 2411, 1130539: 2412, 1130567: 2413, 1130568: 2414, 1130586: 2415, 1130600: 2416, 1130634: 2417, 1130637: 2418, 1130653: 2419, 1130657: 2420, 1130676: 2421, 1130678: 2422, 1130680: 2423, 1130780: 2424, 1130808: 2425, 1130847: 2426, 1130856: 2427, 1130915: 2428, 1130927: 2429, 1130929: 2430, 1130931: 2431, 1130934: 2432, 1131116: 2433, 1131137: 2434, 1131149: 2435, 1131150: 2436, 1131163: 2437, 1131164: 2438, 1131165: 2439, 1131167: 2440, 1131172: 2441, 1131180: 2442, 1131184: 2443, 1131189: 2444, 1131192: 2445, 1131195: 2446, 1131198: 2447, 1131223: 2448, 1131230: 2449, 1131236: 2450, 1131257: 2451, 1131258: 2452, 1131266: 2453, 1131267: 2454, 1131270: 2455, 1131274: 2456, 1131277: 2457, 1131300: 2458, 1131301: 2459, 1131305: 2460, 1131312: 2461, 1131314: 2462, 1131330: 2463, 1131334: 2464, 1131335: 2465, 1131345: 2466, 1131348: 2467, 1131359: 2468, 1131360: 2469, 1131374: 2470, 1131414: 2471, 1131420: 2472, 1131421: 2473, 1131464: 2474, 1131466: 2475, 1131471: 2476, 1131549: 2477, 1131550: 2478, 1131557: 2479, 1131565: 2480, 1131607: 2481, 1131611: 2482, 1131634: 2483, 1131639: 2484, 1131647: 2485, 1131719: 2486, 1131728: 2487, 1131734: 2488, 1131741: 2489, 1131745: 2490, 1131748: 2491, 1131752: 2492, 1131754: 2493, 1131828: 2494, 1132073: 2495, 1132083: 2496, 1132157: 2497, 1132285: 2498, 1132385: 2499, 1132406: 2500, 1132416: 2501, 1132418: 2502, 1132434: 2503, 1132443: 2504, 1132459: 2505, 1132461: 2506, 1132486: 2507, 1132505: 2508, 1132706: 2509, 1132731: 2510, 1132809: 2511, 1132815: 2512, 1132857: 2513, 1132864: 2514, 1132887: 2515, 1132922: 2516, 1132948: 2517, 1132968: 2518, 1133004: 2519, 1133008: 2520, 1133010: 2521, 1133028: 2522, 1133047: 2523, 1133196: 2524, 1133338: 2525, 1133390: 2526, 1133417: 2527, 1133428: 2528, 1133469: 2529, 1133846: 2530, 1133930: 2531, 1134022: 2532, 1134031: 2533, 1134056: 2534, 1134197: 2535, 1134320: 2536, 1134346: 2537, 1134348: 2538, 1134865: 2539, 1135082: 2540, 1135108: 2541, 1135115: 2542, 1135122: 2543, 1135125: 2544, 1135137: 2545, 1135345: 2546, 1135358: 2547, 1135368: 2548, 1135455: 2549, 1135589: 2550, 1135746: 2551, 1135750: 2552, 1135894: 2553, 1135899: 2554, 1135955: 2555, 1136040: 2556, 1136110: 2557, 1136310: 2558, 1136342: 2559, 1136393: 2560, 1136397: 2561, 1136422: 2562, 1136442: 2563, 1136446: 2564, 1136447: 2565, 1136449: 2566, 1136631: 2567, 1136634: 2568, 1136791: 2569, 1136814: 2570, 1137140: 2571, 1137466: 2572, 1138027: 2573, 1138043: 2574, 1138091: 2575, 1138619: 2576, 1138755: 2577, 1138968: 2578, 1138970: 2579, 1139009: 2580, 1139195: 2581, 1139928: 2582, 1140040: 2583, 1140230: 2584, 1140231: 2585, 1140289: 2586, 1140543: 2587, 1140547: 2588, 1140548: 2589, 1152075: 2590, 1152143: 2591, 1152150: 2592, 1152162: 2593, 1152179: 2594, 1152194: 2595, 1152244: 2596, 1152259: 2597, 1152272: 2598, 1152277: 2599, 1152290: 2600, 1152307: 2601, 1152308: 2602, 1152358: 2603, 1152379: 2604, 1152394: 2605, 1152421: 2606, 1152436: 2607, 1152448: 2608, 1152490: 2609, 1152508: 2610, 1152564: 2611, 1152569: 2612, 1152633: 2613, 1152663: 2614, 1152673: 2615, 1152676: 2616, 1152711: 2617, 1152714: 2618, 1152740: 2619, 1152761: 2620, 1152821: 2621, 1152858: 2622, 1152859: 2623, 1152896: 2624, 1152904: 2625, 1152910: 2626, 1152917: 2627, 1152944: 2628, 1152958: 2629, 1152959: 2630, 1152975: 2631, 1152991: 2632, 1153003: 2633, 1153014: 2634, 1153024: 2635, 1153031: 2636, 1153056: 2637, 1153064: 2638, 1153065: 2639, 1153091: 2640, 1153097: 2641, 1153101: 2642, 1153106: 2643, 1153148: 2644, 1153150: 2645, 1153160: 2646, 1153166: 2647, 1153169: 2648, 1153183: 2649, 1153195: 2650, 1153254: 2651, 1153262: 2652, 1153264: 2653, 1153275: 2654, 1153280: 2655, 1153287: 2656, 1153577: 2657, 1153703: 2658, 1153724: 2659, 1153728: 2660, 1153736: 2661, 1153784: 2662, 1153786: 2663, 1153811: 2664, 1153816: 2665, 1153853: 2666, 1153860: 2667, 1153861: 2668, 1153866: 2669, 1153877: 2670, 1153879: 2671, 1153889: 2672, 1153891: 2673, 1153896: 2674, 1153897: 2675, 1153899: 2676, 1153900: 2677, 1153922: 2678, 1153933: 2679, 1153942: 2680, 1153943: 2681, 1153945: 2682, 1153946: 2683, 1154012: 2684, 1154042: 2685, 1154068: 2686, 1154071: 2687, 1154074: 2688, 1154076: 2689, 1154103: 2690, 1154123: 2691, 1154124: 2692, 1154169: 2693, 1154173: 2694, 1154176: 2695, 1154229: 2696, 1154230: 2697, 1154232: 2698, 1154233: 2699, 1154251: 2700, 1154276: 2701, 1154459: 2702, 1154500: 2703, 1154520: 2704, 1154524: 2705, 1154525: 2706, 1155073: 2707}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write mappings back\n",
        "papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
        "citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
        "citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
        "papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])\n"
      ],
      "metadata": {
        "id": "8-4H5QnArxSi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(papers)\n",
        "print(citations)"
      ],
      "metadata": {
        "id": "6X__y1Kiwa-F",
        "outputId": "4b93fb3b-3106-41cd-94ac-9457b19b5fc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      paper_id  term_0  term_1  ...  term_1431  term_1432  subject\n",
            "0          462       0       0  ...          0          0        2\n",
            "1         1911       0       0  ...          0          0        5\n",
            "2         2002       0       0  ...          0          0        4\n",
            "...        ...     ...     ...  ...        ...        ...      ...\n",
            "2705      2372       0       0  ...          0          0        1\n",
            "2706       955       0       0  ...          0          0        0\n",
            "2707       376       0       0  ...          0          0        2\n",
            "\n",
            "[2708 rows x 1435 columns]\n",
            "      target  source\n",
            "0          0      21\n",
            "1          0     905\n",
            "2          0     906\n",
            "...      ...     ...\n",
            "5426    1874    2586\n",
            "5427    1876    1874\n",
            "5428    1897    2707\n",
            "\n",
            "[5429 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "rOE7L16TwfPy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h1nRMpHzqmX_"
      },
      "outputs": [],
      "source": [
        "# Obtain random indices\n",
        "random_indices = np.random.permutation(range(papers.shape[0]))\n",
        "\n",
        "# 50/50 split\n",
        "train_ratio = 0.5\n",
        "train_idx = int(len(random_indices) * train_ratio)\n",
        "train_data = papers.iloc[random_indices[:train_idx]]\n",
        "test_data = papers.iloc[random_indices[train_idx:]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data)\n",
        "print(test_data)"
      ],
      "metadata": {
        "id": "7pLB9adcxMHt",
        "outputId": "0d49d25a-14ca-425f-f707-762deef62195",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      paper_id  term_0  term_1  ...  term_1431  term_1432  subject\n",
            "1044      2279       0       0  ...          0          0        2\n",
            "439        795       0       0  ...          0          0        2\n",
            "1729      1054       0       0  ...          0          0        1\n",
            "...        ...     ...     ...  ...        ...        ...      ...\n",
            "2291       615       0       0  ...          0          0        5\n",
            "338       2356       0       0  ...          0          0        2\n",
            "1706       909       0       0  ...          0          0        0\n",
            "\n",
            "[1354 rows x 1435 columns]\n",
            "      paper_id  term_0  term_1  ...  term_1431  term_1432  subject\n",
            "2252      1491       0       0  ...          0          0        1\n",
            "2271      2288       0       0  ...          0          0        1\n",
            "1501      2269       0       0  ...          0          0        2\n",
            "...        ...     ...     ...  ...        ...        ...      ...\n",
            "1130      1255       0       0  ...          0          0        1\n",
            "1294      1381       0       0  ...          0          0        3\n",
            "860        692       0       0  ...          0          0        1\n",
            "\n",
            "[1354 rows x 1435 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d-wlvqyUqmYA"
      },
      "outputs": [],
      "source": [
        "train_indices = train_data[\"paper_id\"].to_numpy()\n",
        "test_indices = test_data[\"paper_id\"].to_numpy()\n",
        "train_labels = train_data[\"subject\"].to_numpy()\n",
        "test_labels = test_data[\"subject\"].to_numpy()\n",
        "\n",
        "# Define graph, namely an edge tensor and a node feature tensor\n",
        "feature_start = 1\n",
        "feature_end = -1\n",
        "edges = tf.convert_to_tensor(citations[[\"target\", \"source\"]])\n",
        "node_states = tf.convert_to_tensor(\n",
        "    papers.sort_values(\"paper_id\").iloc[:, feature_start:feature_end]\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Edges shape: {edges.shape}\")\n",
        "print(f\"Node features shape: {node_states.shape}\")"
      ],
      "metadata": {
        "id": "03ei-mgzx8l_",
        "outputId": "767df7bb-f459-452f-ac6b-cfb69ad92209",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edges shape: (5429, 2)\n",
            "Node features shape: (2708, 1433)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssG9EI6eqmYB"
      },
      "source": [
        "# Building the model\n",
        "\n",
        "GAT takes as input a graph (namely an edge tensor and a node feature tensor) and\n",
        "outputs \\[updated\\] node states. The node states are, for each target node, neighborhood\n",
        "aggregated information of *N*-hops (where *N* is decided by the number of layers of the\n",
        "GAT). Importantly, in contrast to the\n",
        "[graph convolutional network](https://arxiv.org/abs/1609.02907) (GCN)\n",
        "the GAT makes use of attention machanisms\n",
        "to aggregate information from neighboring nodes (or *source nodes*). In other words, instead of simply\n",
        "averaging/summing node states from source nodes (*source papers*) to the target node (*target papers*),\n",
        "GAT first applies normalized attention scores to each source node state and then sums."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71paunNJqmYB"
      },
      "source": [
        "### (Multi-head) graph attention layer\n",
        "\n",
        "The GAT model implements multi-head graph attention layers. The `MultiHeadGraphAttention` layer is simply a concatenation (or averaging) of multiple graph attention layers (`GraphAttention`), each with separate learnable weights `W`. The `GraphAttention` layer does the following:\n",
        "\n",
        "1. Linear transfrom input node states with W to give Z.\n",
        "```\n",
        "node_states_transformed = tf.matmul(node_states, self.kernel)\n",
        "```\n",
        "2. Computes pair-wise attention scores.\n",
        "```\n",
        "node_states_expanded = tf.gather(node_states_transformed, edges)\n",
        "node_states_expanded = tf.reshape(node_states_expanded, (tf.shape(edges)[0], -1))\n",
        "attention_scores = tf.nn.leaky_relu(tf.matmul(node_states_expanded, self.kernel_attention))\n",
        "attention_scores = tf.squeeze(attention_scores, -1)\n",
        "```\n",
        "\n",
        "2. Normalizes attention scores.\n",
        "```\n",
        "attention_scores = tf.math.exp(tf.clip_by_value(attention_scores, -2, 2))\n",
        "attention_scores_sum = tf.math.unsorted_segment_sum(\n",
        "      data=attention_scores,\n",
        "      segment_ids=edges[:, 0], # Incoming nodes\n",
        "      num_segments=tf.reduce_max(edges[:, 0]) + 1 # node index start with 0\n",
        ")\n",
        "attention_scores_sum = tf.repeat(attention_scores_sum, tf.math.bincount(tf.cast(edges[:, 0], \"int32\")))\n",
        "attention_scores_norm = attention_scores / attention_scores_sum\n",
        "```\n",
        "4. Applies attention scores  and adds it to the new target node state.\n",
        "```\n",
        "node_states_neighbors = tf.gather(node_states_transformed, edges[:, 1])\n",
        "out = tf.math.unsorted_segment_sum(\n",
        "        data=node_states_neighbors * attention_scores_norm[:, tf.newaxis],\n",
        "        segment_ids=edges[:, 0], # Incoming nodes\n",
        "        num_segments=tf.shape(node_states)[0],\n",
        ")\n",
        "```\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/gather\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/math/unsorted_segment_sum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hgQ3HgzaqmYC"
      },
      "outputs": [],
      "source": [
        "class GraphAttention(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        units,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        kernel_regularizer=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
        "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(input_shape[0][-1], self.units),\n",
        "            trainable=True,\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            name=\"kernel\",\n",
        "        )\n",
        "        self.kernel_attention = self.add_weight(\n",
        "            shape=(self.units * 2, 1),\n",
        "            trainable=True,\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            name=\"kernel_attention\",\n",
        "        )\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        node_states, edges = inputs\n",
        "\n",
        "        # Linear Transform\n",
        "        node_states_transformed = tf.matmul(node_states, self.kernel)\n",
        "\n",
        "        # Compute pair-wise attention scores\n",
        "        node_states_expanded = tf.gather(node_states_transformed, edges)\n",
        "        node_states_expanded = tf.reshape(\n",
        "            node_states_expanded, (tf.shape(edges)[0], -1)\n",
        "        )\n",
        "        attention_scores = tf.nn.leaky_relu(\n",
        "            tf.matmul(node_states_expanded, self.kernel_attention)\n",
        "        )\n",
        "        attention_scores = tf.squeeze(attention_scores, -1)\n",
        "\n",
        "        # Normalize attention scores\n",
        "        attention_scores = tf.math.exp(\n",
        "            tf.clip_by_value(attention_scores, -2, 2)\n",
        "        )\n",
        "        attention_scores_sum = tf.math.unsorted_segment_sum(\n",
        "            data=attention_scores,\n",
        "            segment_ids=edges[:, 0],\n",
        "            num_segments=tf.reduce_max(edges[:, 0]) + 1,\n",
        "        )\n",
        "        attention_scores_sum = tf.repeat(\n",
        "            attention_scores_sum,\n",
        "            tf.math.bincount(tf.cast(edges[:, 0], \"int32\"))\n",
        "        )\n",
        "        attention_scores_norm = attention_scores / attention_scores_sum\n",
        "\n",
        "        # Gather node states of neighbors, apply attention scores and aggregate\n",
        "        node_states_neighbors = tf.gather(node_states_transformed, edges[:, 1])\n",
        "        out = tf.math.unsorted_segment_sum(\n",
        "            data=node_states_neighbors * attention_scores_norm[:, tf.newaxis],\n",
        "            segment_ids=edges[:, 0],\n",
        "            num_segments=tf.shape(node_states)[0],\n",
        "        )\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadGraphAttention(layers.Layer):\n",
        "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.merge_type = merge_type\n",
        "        self.attention_layers = [GraphAttention(units) for _ in range(num_heads)]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        atom_features, pair_indices = inputs\n",
        "\n",
        "        # Obtain outputs from each attention head\n",
        "        outputs = [\n",
        "            attention_layer([atom_features, pair_indices])\n",
        "            for attention_layer in self.attention_layers\n",
        "        ]\n",
        "\n",
        "        # Concatenate or average the node states from each head\n",
        "        if self.merge_type == \"concat\":\n",
        "            outputs = tf.concat(outputs, axis=-1)\n",
        "        else:\n",
        "            outputs = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
        "        \n",
        "        # Activate and return node states\n",
        "        return tf.nn.relu(outputs)"
      ],
      "metadata": {
        "id": "8InouGDG3JQW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfUYHGbBqmYC"
      },
      "source": [
        "Note that the GAT model operates on the entire graph in all phases (training, validation and testing).\n",
        "\n",
        "The difference between the phases are the indices (and labels), which gathers certain outputs (`tf.gather(outputs, indices)`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jK606ENRqmYC"
      },
      "outputs": [],
      "source": [
        "class GraphAttentionNetwork(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        node_states,\n",
        "        edges,\n",
        "        hidden_units,\n",
        "        num_heads,\n",
        "        num_layers,\n",
        "        output_dim,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.node_states = node_states\n",
        "        self.edges = edges\n",
        "        self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")\n",
        "        self.attention_layers = [\n",
        "            MultiHeadGraphAttention(hidden_units, num_heads) for _ in range(num_layers)\n",
        "        ]\n",
        "        self.output_layer = layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        node_states, edges = inputs\n",
        "        x = self.preprocess(node_states)\n",
        "        for attention_layer in self.attention_layers:\n",
        "            x = attention_layer([x, edges]) + x\n",
        "        outputs = self.output_layer(x)\n",
        "        return outputs\n",
        "\n",
        "    def train_step(self, data):\n",
        "\n",
        "        indices, labels = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            \n",
        "            outputs = self([self.node_states, self.edges])            \n",
        "            loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
        "        \n",
        "        grads = tape.gradient(loss, self.trainable_weights)        \n",
        "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        \n",
        "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def predict_step(self, data):\n",
        "        \n",
        "        indices = data        \n",
        "        outputs = self([self.node_states, self.edges])\n",
        "        \n",
        "        return tf.nn.softmax(tf.gather(outputs, indices))\n",
        "\n",
        "    def test_step(self, data):\n",
        "        \n",
        "        indices, labels = data\n",
        "        \n",
        "        outputs = self([self.node_states, self.edges])        \n",
        "        loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
        "        \n",
        "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox4TnPKgqmYD"
      },
      "source": [
        "# Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_UNITS = 100\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 3\n",
        "OUTPUT_DIM = len(class_values)\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 256\n",
        "VALIDATION_SPLIT = 0.1\n",
        "LEARNING_RATE = 3e-1\n",
        "MOMENTUM = 0.9"
      ],
      "metadata": {
        "id": "oPYrYS2o4_Xh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qfBwzkWjqmYD",
        "outputId": "eedf5855-4924-4634-bce5-fa6d9e42848d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "5/5 - 48s - loss: 1.8605 - acc: 0.2693 - val_loss: 1.5593 - val_acc: 0.3676 - 48s/epoch - 10s/step\n",
            "Epoch 2/100\n",
            "5/5 - 6s - loss: 1.2725 - acc: 0.5411 - val_loss: 1.1496 - val_acc: 0.6029 - 6s/epoch - 1s/step\n",
            "Epoch 3/100\n",
            "5/5 - 6s - loss: 0.7648 - acc: 0.7447 - val_loss: 0.8545 - val_acc: 0.6912 - 6s/epoch - 1s/step\n",
            "Epoch 4/100\n",
            "5/5 - 6s - loss: 0.4805 - acc: 0.8604 - val_loss: 0.8469 - val_acc: 0.6985 - 6s/epoch - 1s/step\n",
            "Epoch 5/100\n",
            "5/5 - 6s - loss: 0.3052 - acc: 0.9130 - val_loss: 0.7723 - val_acc: 0.7500 - 6s/epoch - 1s/step\n",
            "Epoch 6/100\n",
            "5/5 - 6s - loss: 0.1890 - acc: 0.9532 - val_loss: 0.8160 - val_acc: 0.7353 - 6s/epoch - 1s/step\n",
            "Epoch 7/100\n",
            "5/5 - 6s - loss: 0.1204 - acc: 0.9762 - val_loss: 0.8122 - val_acc: 0.7647 - 6s/epoch - 1s/step\n",
            "Epoch 8/100\n",
            "5/5 - 6s - loss: 0.0722 - acc: 0.9893 - val_loss: 0.8209 - val_acc: 0.7500 - 6s/epoch - 1s/step\n",
            "Epoch 9/100\n",
            "5/5 - 6s - loss: 0.0477 - acc: 0.9951 - val_loss: 0.8949 - val_acc: 0.7132 - 6s/epoch - 1s/step\n",
            "Epoch 10/100\n",
            "5/5 - 6s - loss: 0.0375 - acc: 0.9959 - val_loss: 0.8271 - val_acc: 0.7574 - 6s/epoch - 1s/step\n",
            "Epoch 11/100\n",
            "5/5 - 6s - loss: 0.0285 - acc: 0.9959 - val_loss: 0.8417 - val_acc: 0.7647 - 6s/epoch - 1s/step\n",
            "Epoch 12/100\n",
            "5/5 - 6s - loss: 0.0194 - acc: 0.9975 - val_loss: 0.9454 - val_acc: 0.7426 - 6s/epoch - 1s/step\n",
            "\n",
            "Test Accuracy 80.1%\n"
          ]
        }
      ],
      "source": [
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = keras.optimizers.SGD(LEARNING_RATE, momentum=MOMENTUM)\n",
        "accuracy_fn = keras.metrics.SparseCategoricalAccuracy(name=\"acc\")\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_acc\", min_delta=1e-5, patience=5, restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Build model\n",
        "gat_model = GraphAttentionNetwork(\n",
        "    node_states, edges, HIDDEN_UNITS, NUM_HEADS, NUM_LAYERS, OUTPUT_DIM\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "gat_model.compile(loss=loss_fn, optimizer=optimizer, metrics=[accuracy_fn])\n",
        "\n",
        "gat_model.fit(\n",
        "    x=train_indices,\n",
        "    y=train_labels,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "_, test_accuracy = gat_model.evaluate(x=test_indices, y=test_labels, verbose=0)\n",
        "\n",
        "print(f\"\\nTest Accuracy {test_accuracy*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "u80DwREHqmYE",
        "outputId": "02c5eaea-f50a-44d4-b3be-7655e2d18215",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1: Truth = Genetic_Algorithms, Prediction = Genetic_Algorithms, 89.912%\n",
            "Example 2: Truth = Genetic_Algorithms, Prediction = Genetic_Algorithms, 99.078%\n",
            "Example 3: Truth = Neural_Networks, Prediction = Neural_Networks, 79.022%\n",
            "Example 4: Truth = Probabilistic_Methods, Prediction = Probabilistic_Methods, 96.528%\n",
            "Example 5: Truth = Probabilistic_Methods, Prediction = Theory, 56.725%\n",
            "Example 6: Truth = Neural_Networks, Prediction = Neural_Networks, 98.823%\n",
            "Example 7: Truth = Neural_Networks, Prediction = Neural_Networks, 100.000%\n",
            "Example 8: Truth = Reinforcement_Learning, Prediction = Theory, 31.685%\n",
            "Example 9: Truth = Probabilistic_Methods, Prediction = Neural_Networks, 99.796%\n",
            "Example 10: Truth = Theory, Prediction = Theory, 86.504%\n"
          ]
        }
      ],
      "source": [
        "test_probs = gat_model.predict(x=test_indices)\n",
        "\n",
        "mapping = {v: k for (k, v) in class_idx.items()}\n",
        "\n",
        "for i, (probs, label) in enumerate(zip(test_probs[:10], test_labels[:10])):\n",
        "    pred = np.argmax(probs)\n",
        "    print(f\"Example {i+1}: Truth = {mapping[label]}, Prediction = {mapping[pred]}, {probs[pred]:7.3%}\")\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "gat_node_classification",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}